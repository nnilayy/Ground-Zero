{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "R6DXuEhE4GYS",
        "ODkd18rA2n4S",
        "lXP3i2-3_iuB"
      ],
      "authorship_tag": "ABX9TyP9e2dRaiJhvycykjVkPS70",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/Unet/blob/main/Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "esJE2uN6zlsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "img=read_image(\"/content/Images/1.jpeg\")\n",
        "cv2_imshow(img*255)\n",
        "img.shape"
      ],
      "metadata": {
        "id": "IIcTGWjyk2la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Preprocessing"
      ],
      "metadata": {
        "id": "R6DXuEhE4GYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob \n",
        "import numpy as np\n",
        "X_train=[]\n",
        "y_train=[]\n",
        "images=sorted(glob.glob(\"/content/drive/MyDrive/Augmented/Images/*\"))\n",
        "masks=sorted(glob.glob(\"/content/drive/MyDrive/Augmented/Masks/*\"))\n",
        "\n",
        "\n",
        "for image,mask in zip(images,masks):\n",
        "  img=cv2.imread(image)\n",
        "  img=np.array(img)\n",
        "  img=np.resize(img,(1,256,256,3))\n",
        "  X_train.append(img)\n",
        " \n",
        "  msk=cv2.imread(mask)\n",
        "  msk=np.array(msk)\n",
        "  msk=np.resize(msk,(1,256,256,1))\n",
        "  y_train.append(msk)\n",
        "\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "\n",
        "# X_train=cv2.imread(\"/content/drive/MyDrive/Augmented/Images/0_0_4958.jpeg\")\n",
        "# X_train=np.resize(X_train,(1,256,256,3))\n",
        "# X_train=tf.expand_dims(X_train, axis=0)\n",
        "# y_train=cv2.imread(\"/content/drive/MyDrive/Augmented/Masks/0_0_4958.jpeg\")\n",
        "# y_train=np.resize(y_train,(1,256,256,1))\n",
        "# y_train=tf.expand_dims(y_train, axis=0)\n"
      ],
      "metadata": {
        "id": "ZChbB-ovF13Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "ODkd18rA2n4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(inputs,num_filters):\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(inputs)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def encoder_block(inputs,num_filters):\n",
        "  x=conv_block(inputs,num_filters)\n",
        "  p=MaxPool2D((2,2))(x)\n",
        "  return x,p\n",
        "\n",
        "def decoder_block(inputs, skip_features,num_filters):\n",
        "  x=Conv2DTranspose(num_filters,(2,2),strides=2,padding=\"same\")(inputs)\n",
        "  x=Concatenate()([x,skip_features])\n",
        "  x=conv_block(x,num_filters)\n",
        "  return x\n",
        "\n",
        "def unet(input_shape):\n",
        "  inputs=Input(input_shape)\n",
        "\n",
        "  s1,p1=encoder_block(inputs,64)\n",
        "  s2,p2=encoder_block(p1,128)\n",
        "  s3,p3=encoder_block(p2,256)\n",
        "  s4,p4=encoder_block(p3,512)\n",
        "\n",
        "  b1 = conv_block(p4,1024)\n",
        "\n",
        "  d1 = decoder_block(b1,s4,512)\n",
        "  d2 = decoder_block(d1,s3,256)\n",
        "  d3 = decoder_block(d2,s2,128)\n",
        "  d4 = decoder_block(d3,s1,64)\n",
        "\n",
        "  outputs=Conv2D(1,(1,1),padding=\"same\",activation=\"sigmoid\")(d4)\n",
        "  model=Model(inputs,outputs,name=\"U-Net\")\n",
        "  return model  "
      ],
      "metadata": {
        "id": "5D6TluvZqSbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Summary\n",
        "model=unet((256,256,3))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "8PkqYSaC6gdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# Model\n",
        "input_shape=(256,256,3)\n",
        "model=unet(input_shape)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# model.compile()\n",
        "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "# model.compile(loss=dice_loss, optimizer=Adam(1e-4),metrics=[dice_coef])\n",
        "# model.compile(loss=dice_loss,optimizer=Adam(),metrics=metrics)\n",
        "# metrics=[dice_coef,iou,Recall(),Precision()]\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# Callbacks\n",
        "# callbacks=[tf.keras.callbacks.EarlyStopping(patience=15,monitor=\"val_loss\"),\n",
        "#             tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\"),\n",
        "#             tf.keras.callbacks.ModelCheckpoint(\"model_for_balls.h5\",save_best_only=True)\n",
        "#             tf.keras.callbacks.ReduceLROnPlateau()]\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# model.fit()\n",
        "# results=model.fit(X_train,y_train,validation_split=0.1,batch_size=16,epochs=100,callbacks=callbacks)\n",
        "# model.fit(train_dataset,epochs=20,validation_data=valid_dataset,callbacks=callbacks)\n",
        "train_generator=zip(X_train,y_train)\n",
        "val_generator=zip(X_val,y_val)\n",
        "batch_size=16\n",
        "model.fit(train_generator,steps_per_epoch=60//batch_size,batch_size=batch_size, epochs=10,callbacks=tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\"))\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "#ReduceLROnPlateau[49:58]: https://www.youtube.com/watch?v=lstBIXVUoSM\n",
        "# model.save(\"/content/drive/MyDrive/Birds_Dataset_Main/Models/VGG19_Adam.h5\")"
      ],
      "metadata": {
        "id": "nzjXl5p7fSbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard"
      ],
      "metadata": {
        "id": "lXP3i2-3_iuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/logs"
      ],
      "metadata": {
        "id": "f3X8lq1ANP5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Models"
      ],
      "metadata": {
        "id": "I_DAWWLrimNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def unet(pretrained_weights=None, input_size=(256,256,3)):\n",
        "  inputs=Input(input_size)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Encoder Block\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(inputs)\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv1)\n",
        "  pool1=MaxPool2D((2,2))(conv1)\n",
        "\n",
        "  conv2=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool1)\n",
        "  conv2=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv2)\n",
        "  pool2=MaxPool2D((2,2))(conv2)\n",
        "\n",
        "  conv3=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool2)\n",
        "  conv3=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv3)\n",
        "  pool3=MaxPool2D((2,2))(conv3)\n",
        "\n",
        "  conv4=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool3)\n",
        "  conv4=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv4)\n",
        "  pool4=MaxPool2D((2,2))(conv4)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Bottleneck\n",
        "  conv5=Conv2D(1024,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool4)\n",
        "  conv5=Conv2D(1024,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv5)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Decoder Block\n",
        "  up6=Conv2DTranspose(512,(2,2),strides=2,padding=\"same\")(conv5)\n",
        "  merge6=Concatenate()([up6,conv5],axis=3)\n",
        "  conv6=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge6)\n",
        "  conv6=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv6)\n",
        "\n",
        "  up7=Conv2DTranspose(256,(2,2),strides=2,padding=\"same\")(conv6)\n",
        "  merge7=Concatenate()([up7,conv3],axis=3)\n",
        "  conv7=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge7)\n",
        "  conv7=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv7)\n",
        "\n",
        "  up8=Conv2DTranspose(128,(2,2),strides=2,padding=\"same\")(conv7)\n",
        "  merge8=Concatenate()([up8,conv2],axis=3)\n",
        "  conv8=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge8)\n",
        "  conv8=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv8)\n",
        "\n",
        "  up9=Conv2DTranspose(64,(2,2),strides=2,padding=\"same\")(conv8)\n",
        "  merge9=Concatenate()([up8,conv2],axis=3)\n",
        "  conv9=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge9)\n",
        "  conv9=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv9)\n",
        "\n",
        "  conv10=Conv2D(1,1,activation=\"sigmoid\")(conv9)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "  model=Model(input=inputs, output=conv10)\n",
        "  model.compile(optimizer=Adam(lr=1e-4),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "-Kl5n1bex5RS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# You Can Add BatchNormalization Layers\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(inputs)\n",
        "  conv1=BatchNormalization()(conv1)\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv1)\n",
        "  conv1=BatchNormalization()(conv1)\n",
        "  pool1=MaxPool2D((2,2))(conv1)\n",
        "\n",
        "# You Can Add Dropout layers too\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(inputs)\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv1)\n",
        "  conv1=Dropout(0.2)(conv1) "
      ],
      "metadata": {
        "id": "ZtcrmBZt4Pp2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import tensorflow as tf \n",
        "IMG_WIDTH=128\n",
        "IMG_HEIGHT=128\n",
        "IMG_CHANNEL=3\n",
        "\n",
        "inputs=tf.keras.layers.Input((IMG_WIDTH,IMG_HEIGHT,IMG_CHANNEL))\n",
        "s=tf.keras.layers.Lambda(lambda x:x/255)(inputs)\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# CONTRACTION PATH \n",
        "# --------------------------------------------------------------------------------------------\n",
        "c1=tf.keras.layers.Conv2D(16,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(inputs)\n",
        "c1=tf.keras.layers.Dropout(0.1)(c1)\n",
        "c1=tf.keras.layers.Conv2D(16,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(c1)\n",
        "p1=tf.keras.layers.MaxPooling2D((2,2))(c1)\n",
        "\n",
        "c2=tf.keras.layers.Conv2D(32,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(p1)\n",
        "c2=tf.keras.layers.Dropout(0.1)(c2)\n",
        "c2=tf.keras.layers.Conv2D(32,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(c2)\n",
        "p2=tf.keras.layers.MaxPooling2D((2,2))(c2)\n",
        "\n",
        "c3=tf.keras.layers.Conv2D(64,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(p2)\n",
        "c3=tf.keras.layers.Dropout(0.1)(c3)\n",
        "c3=tf.keras.layers.Conv2D(64,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(c3)\n",
        "p3=tf.keras.layers.MaxPooling2D((2,2))(c3)\n",
        "\n",
        "c4=tf.keras.layers.Conv2D(128,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(p3)\n",
        "c4=tf.keras.layers.Dropout(0.1)(c4)\n",
        "c4=tf.keras.layers.Conv2D(128,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(c4)\n",
        "p4=tf.keras.layers.MaxPooling2D((2,2))(c4)\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# BOTTLENECK\n",
        "# --------------------------------------------------------------------------------------------\n",
        "c5=tf.keras.layers.Conv2D(256,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(p4)\n",
        "c5=tf.keras.layers.Dropout(0.1)(c5)\n",
        "c5=tf.keras.layers.Conv2D(256,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(c5)\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# EXPANSION PATH\n",
        "# --------------------------------------------------------------------------------------------\n",
        "u6=tf.keras.layers.Conv2DTranspose(128,(2,2),strides=2,padding=\"same\")(c5)\n",
        "u6=tf.keras.layers.concatenate([u6,c4])  \n",
        "c6=tf.keras.layers.Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(u6)\n",
        "c6=tf.keras.layers.Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(c6)\n",
        "\n",
        "u7=tf.keras.layers.Conv2DTranspose(64,(2,2),strides=2,padding=\"same\")(c6)\n",
        "u7=tf.keras.layers.concatenate([u7,c3])\n",
        "c7=tf.keras.layers.Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(u7)\n",
        "c7=tf.keras.layers.Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(c7)\n",
        "\n",
        "u8=tf.keras.layers.Conv2DTranspose(23,(2,2),strides=2,padding=\"same\")(c7)\n",
        "u8=tf.keras.layers.concatenate([u8,c2])\n",
        "c8=tf.keras.layers.Conv2D(32,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(u8)\n",
        "c8=tf.keras.layers.Conv2D(32,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(c8)\n",
        "\n",
        "u9=tf.keras.layers.Conv2DTranspose(16,(2,2),strides=2,padding=\"same\")(c8)\n",
        "u9=tf.keras.layers.concatenate([u9,c1])\n",
        "c9=tf.keras.layers.Conv2D(16,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(u9)\n",
        "c9=tf.keras.layers.Conv2D(16,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(c9)\n",
        "\n",
        "outputs=tf.keras.layers.Conv2D(1,1,activation=\"sigmoid\")(c9)\n",
        "\n",
        "model=tf.keras.Model(inputs=[inputs],outputs=[outputs])\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "z2bMxmgogTw9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Getting Dataset flow ready\n",
        "# ()Build a Model\n",
        "# ()Test if it works \n",
        "# ()Can it be deployed on Spresense \n",
        "# ()If so what are the results"
      ],
      "metadata": {
        "id": "ZKw192PJpErR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Removing and Adding Filter layers reduces params\n",
        "# Train the model\n",
        "# What is size of the weight file in the end\n",
        "# What is weight file after converting to tflite file\n",
        "# Can it run on Spresense \n",
        "# Can it be quantized further\n",
        "\n",
        "# 11:00-78"
      ],
      "metadata": {
        "id": "Ns6RJP_BpGBR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# ()Data analysis Project Repo\n",
        "# ()Read Me for Different projects\n",
        "# ()Movie Recommendation System"
      ],
      "metadata": {
        "id": "w8BrvFEKoqOR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u8Mbe098Afp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from akida_models import akidanet_imagenet\n",
        "from keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, Softmax, ReLU\n",
        "from cnn2snn import check_model_compatibility\n",
        "from ei_tensorflow.constrained_object_detection import models, dataset, metrics, util\n",
        "\n",
        "def build_model(input_shape: tuple, alpha: float,num_classes: int, weight_regularizer=None) -> tf.keras.Model:\n",
        "    \"\"\" Construct a constrained object detection model.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Passed to AkidaNet construction.\n",
        "        alpha: AkidaNet alpha value.\n",
        "        num_classes: Number of classes, i.e. final dimension size, in output.\n",
        "\n",
        "    Returns:\n",
        "        Uncompiled keras model.\n",
        "\n",
        "    Model takes (B, H, W, C) input and\n",
        "    returns (B, H//8, W//8, num_classes) logits.\n",
        "    \"\"\"\n",
        "    #! Create a quantized base model without top layers\n",
        "    a_base_model = akidanet_imagenet(input_shape=input_shape,alpha=alpha,include_top=False,input_scaling=None)\n",
        "    #! Get pretrained quantized weights and load them into the base model\n",
        "    #! Available base models are:\n",
        "    #! akidanet_imagenet_224_alpha_50.h5             - float32 model, 224x224x3, alpha=0.5\n",
        "    #! akidanet_imagenet_160_alpha_50.h5             - float32 model, 160x160x3, alpha=0.5\n",
        "    pretrained_weights = './transfer-learning-weights/akidanet/akidanet_imagenet_224_alpha_50.h5'\n",
        "    a_base_model.load_weights(pretrained_weights, by_name=True, skip_mismatch=True)\n",
        "    a_base_model.trainable = True\n",
        "    #! Default batch norm is configured for huge networks, let's speed it up\n",
        "    for layer in a_base_model.layers:\n",
        "        if type(layer) == BatchNormalization:\n",
        "            layer.momentum = 0.9\n",
        "    #! Cut AkidaNet where it hits 1/8th input resolution; i.e. (HW/8, HW/8, C)\n",
        "    a_cut_point = a_base_model.get_layer('separable_5_relu')\n",
        "    #! Now attach a small additional head on the AkidaNet\n",
        "    a_model_part_head = Conv2D(filters=32, kernel_size=1, strides=1, padding='same',kernel_regularizer=weight_regularizer)(a_cut_point.output)\n",
        "    a_model_part = ReLU()(a_model_part_head)\n",
        "    a_logits = Conv2D(filters=num_classes, kernel_size=1, strides=1, padding='same',activation=None, kernel_regularizer=weight_regularizer)(a_model_part)\n",
        "    fomo_akida = Model(inputs=a_base_model.input, outputs=a_logits)\n",
        "    #! Check if the model is sompatbile with Akida (fail quickly before training)\n",
        "    compatible = check_model_compatibility(fomo_akida, input_is_image=True)\n",
        "    if not compatible:\n",
        "        print(\"Model is not compatible with Akida!\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    return fomo_akida\n",
        "\n",
        "def train(num_classes: int, learning_rate: float, num_epochs: int,alpha: float, object_weight: int,train_dataset: tf.data.Dataset,validation_dataset: tf.data.Dataset,best_model_path: str,input_shape: tuple, callbacks: 'list',quantize_function,lr_finder: bool = False) -> tf.keras.Model:\n",
        "    \"\"\" Construct and train a constrained object detection model.\n",
        "\n",
        "    Args:\n",
        "        num_classes: Number of classes in datasets. This does not include\n",
        "        implied background class introduced by segmentation map dataset\n",
        "        conversion.\n",
        "        learning_rate: Learning rate for Adam.\n",
        "        num_epochs: Number of epochs passed to model.fit\n",
        "        alpha: Alpha used to construct AkidaNet. Pretrained weights will be\n",
        "        used if there is a matching set.\n",
        "        object_weight: The weighting to give the object in the loss function\n",
        "            where background has an implied weight of 1.0.\n",
        "        train_dataset: Training dataset of (x, (bbox, one_hot_y))\n",
        "        validation_dataset: Validation dataset of (x, (bbox, one_hot_y))\n",
        "        best_model_path: location to save best model path. note: weights\n",
        "            will be restored from this path based on best val_f1 score.\n",
        "        input_shape: The shape of the model's input\n",
        "        lr_finder: TODO\n",
        "    Returns:\n",
        "        Trained keras model.\n",
        "\n",
        "    Constructs a new constrained object detection model with num_classes+1\n",
        "    outputs (denoting the classes with an implied background class of 0).\n",
        "    Both training and validation datasets are adapted from\n",
        "    (x, (bbox, one_hot_y)) to (x, segmentation_map). Model is trained with a\n",
        "    custom weighted cross entropy function.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    num_classes_with_background = num_classes + 1\n",
        "\n",
        "    input_width_height = None\n",
        "    width, height, input_num_channels = input_shape\n",
        "    if width != height:\n",
        "        raise Exception(f\"Only square inputs are supported; not {input_shape}\")\n",
        "    input_width_height = width\n",
        "\n",
        "    model = build_model(input_shape=input_shape,alpha=alpha,num_classes=num_classes_with_background,weight_regularizer=tf.keras.regularizers.l2(4e-5))\n",
        "    #! Derive output size from model\n",
        "    model_output_shape = model.layers[-1].output.shape\n",
        "    _batch, width, height, num_classes = model_output_shape\n",
        "    if width != height:\n",
        "        raise Exception(f\"Only square outputs are supported; not {model_output_shape}\")\n",
        "    output_width_height = width\n",
        "\n",
        "    #! Build weighted cross entropy loss specific to this model size\n",
        "    weighted_xent = models.construct_weighted_xent_fn(model.output.shape, object_weight)\n",
        "    #! Transform bounding box labels into segmentation maps\n",
        "    train_segmentation_dataset = train_dataset.map(dataset.bbox_to_segmentation(output_width_height, num_classes_with_background)).batch(32, drop_remainder=False).prefetch(1)\n",
        "    validation_segmentation_dataset = validation_dataset.map(dataset.bbox_to_segmentation(output_width_height, num_classes_with_background, validation=True)).batch(32, drop_remainder=False).prefetch(1)\n",
        "    #! Initialise bias of final classifier based on training data prior.\n",
        "    util.set_classifier_biases_from_dataset(model, train_segmentation_dataset)\n",
        "    if lr_finder:\n",
        "        learning_rate = ei_tensorflow.lr_finder.find_lr(model, train_segmentation_dataset, weighted_xent)\n",
        "\n",
        "    opt = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss=weighted_xent,optimizer=opt)\n",
        "\n",
        "    #! Create callback that will do centroid scoring on end of epoch against\n",
        "    #! validation data. Include a callback to show % progress in slow cases.\n",
        "    centroid_callback = metrics.CentroidScoring(validation_segmentation_dataset,output_width_height, num_classes_with_background)\n",
        "    print_callback = metrics.PrintPercentageTrained(num_epochs)\n",
        "\n",
        "    #! Include a callback for model checkpointing based on the best validation f1.\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(best_model_path,monitor='val_f1', save_best_only=True, mode='max',save_weights_only=True, verbose=0)\n",
        "    model.fit(train_segmentation_dataset,validation_data=validation_segmentation_dataset,epochs=num_epochs,callbacks=callbacks + [centroid_callback, print_callback, checkpoint_callback],verbose=0)\n",
        "    #! Restore best weights.\n",
        "    model.load_weights(best_model_path)\n",
        "    #! Add explicit softmax layer before export.\n",
        "    softmax_layer = Softmax()(model.layers[-1].output)\n",
        "    model = Model(model.input, softmax_layer)\n",
        "    #! Check if model is compatible with Akida\n",
        "    compatible = check_model_compatibility(model, input_is_image=True)\n",
        "    if not compatible:\n",
        "        print(\"Model is not compatible with Akida!\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    akida_model = quantize_function(model=model,train_dataset=train_segmentation_dataset,validation_dataset=validation_segmentation_dataset,optimizer=opt,fine_tune_loss=weighted_xent,fine_tune_metrics=None,best_model_path=best_model_path,callbacks=callbacks + [centroid_callback, print_callback],stopping_metric='val_f1',verbose=0)\n",
        "\n",
        "    return model, akida_model\n",
        "\n",
        "\n",
        "EPOCHS = args.epochs or 100\n",
        "LEARNING_RATE = args.learning_rate or 0.001\n",
        "\n",
        "def quantize_brainchip(model,train_dataset: tf.data.Dataset,validation_dataset: tf.data.Dataset,best_model_path: str, optimizer: str,fine_tune_loss: str,fine_tune_metrics: 'list[str]',callbacks, stopping_metric='val_accuracy',verbose=2):\n",
        "    import tensorflow as tf\n",
        "    import cnn2snn\n",
        "\n",
        "    print('Performing post-training quantization...')\n",
        "    akida_model = cnn2snn.quantize(model,weight_quantization=4,activ_quantization=4,input_weight_quantization=8)\n",
        "    print('Performing post-training quantization OK')\n",
        "    print('')\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=stopping_metric,mode='max',verbose=1,min_delta=0,patience=10,restore_best_weights=True)\n",
        "    callbacks.append(early_stopping)\n",
        "\n",
        "    print('Running quantization-aware training...')\n",
        "    akida_model.compile(optimizer=optimizer,loss=fine_tune_loss,metrics=fine_tune_metrics)\n",
        "    akida_model.fit(train_dataset,epochs=30,verbose=verbose,validation_data=validation_dataset,callbacks=callbacks)\n",
        "    print('Running quantization-aware training OK')\n",
        "    print('')\n",
        "\n",
        "    return akida_model\n",
        "\n",
        "\n",
        "model, akida_model = train(num_classes=classes,learning_rate=LEARNING_RATE,num_epochs=EPOCHS,alpha=0.5,object_weight=100,train_dataset=train_dataset,validation_dataset=validation_dataset,best_model_path=BEST_MODEL_PATH,input_shape=MODEL_INPUT_SHAPE,callbacks=callbacks,quantize_function=quantize_brainchip,lr_finder=False)\n",
        "override_mode = 'segmentation'\n",
        "disable_per_channel_quantization = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Based UNET\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KwpvzoaMYz_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import backend as K\n",
        "import cv2\n",
        "H=256\n",
        "W=256\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# iou()\n",
        "def iou(y_true,y_pred):\n",
        "  def f(y_true,y_pred):\n",
        "    intersection=(y_true * y_pred).sum()\n",
        "    union=y_true.sum()+y_pred.sum()-intersection\n",
        "    x=(intersection+1e-15)/(union+1e-15)\n",
        "    x=x.astype(np.float32)\n",
        "    return x\n",
        "  return tf.numpy_function(f,[y_true,y_pred],tf.float32)\n",
        "\n",
        "# dice_coef()\n",
        "smooth=1e-15\n",
        "def dice_coef(y_true,y_pred):\n",
        "  y_true=tf.keras.layers.Flatten()(y_true)\n",
        "  y_pred=tf.keras.layers.Flatten()(y_pred)\n",
        "  intersection=tf.reduce_sum(y_true*y_pred)\n",
        "  return (2. * intersection+smooth)/(tf.reduce_sum(y_true)+tf.reduce_sum(y_pred)+smooth)\n",
        "\n",
        "# dice_loss()\n",
        "def dice_loss(y_true,y_pred):\n",
        "  return 1.0-dice_coef(y_true,y_pred)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# Shuffling()\n",
        "def shuffling(x,y):\n",
        "  x,y=shuffle(x,y,random_state=42)\n",
        "  return x,y\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# load_data()\n",
        "def load_data(path):\n",
        "  x=sorted(glob.glob())\n",
        "  y=sorted(glob.glob())\n",
        "  return x,y\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# X_train, y_train=load_data(train_path)\n",
        "# X_train, y_train=shuffling(X_train, y_train)\n",
        "# X_test, y_test=load_data(test_path)\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "def read_image(path):\n",
        "  # path=path.decode()\n",
        "  x=cv2.imread(path,cv2.IMREAD_COLOR)\n",
        "  x=cv2.resize(x,(H,W))\n",
        "  x=x/255\n",
        "  x=x.astype(np.float32)\n",
        "  return x\n",
        "\n",
        "def read_mask(path):\n",
        "  # path=path.decode()\n",
        "  x=cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
        "  x=cv2.resize(x,(W,H))\n",
        "  x=x/255\n",
        "  x=x>0.5\n",
        "  x=x.astype(np.float32)\n",
        "  x=np.expand_dims(x,axis=-1)\n",
        "  return x\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "def tf_parse(x,y): \n",
        "  def _parse(x,y):\n",
        "    x=read_image(x)\n",
        "    y=read_image(y)\n",
        "    return x, y\n",
        "  x,y=tf.numpy_function(_parse,[x,y],[tf.float32,tf.float32])\n",
        "  x.set_shape([H,W,3])\n",
        "  y.set_shape([H,W,1])\n",
        "  return x,y \n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "def tf_dataset(X,  Y, batch=2):\n",
        "  dataset=tf.data.Dataset.from_tenor_slices((X,Y))\n",
        "  dataset=dataset.map(tf_parse)\n",
        "  dataset=dataset.batch(batch)\n",
        "  dataset=dataset.prefetch(10)\n",
        "  return dataset\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# train_dataset=tf_dataset(x_train,y_train,batch=16)\n",
        "# valid_dataset=tf_dataset(x_val,y_val,batch=16)\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "JHkbF9EF4uKc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EPMRD9LmJdVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800fc68a-a153-4fa7-bf8b-546dd5434a26",
        "id": "pLq5J0Z2JdqV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pLLXqxmJdqW"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "img=read_image(\"/content/Images/1.jpeg\")\n",
        "cv2_imshow(img*255)\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxs-0WnpuL-K"
      },
      "source": [
        "## Unzipping the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjo0ZTV6uLaU"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/Images.zip -d /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIlGyCveJdqW"
      },
      "source": [
        "## Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kELcJYhJdqX"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob \n",
        "import numpy as np\n",
        "X=[]\n",
        "y=[]\n",
        "images=sorted(glob.glob(\"/content/drive/MyDrive/Augmented/Images/Images/*\"))\n",
        "masks=sorted(glob.glob(\"/content/drive/MyDrive/Augmented/Masks/Masks/*\"))\n",
        "\n",
        "\n",
        "for image,mask in zip(images,masks):\n",
        "  img=cv2.imread(image)\n",
        "  img=np.array(img)\n",
        "  img=np.resize(img,(256,256,1))\n",
        "  X.append(img)\n",
        " \n",
        "  msk=cv2.imread(mask)\n",
        "  msk=np.array(msk)\n",
        "  msk=np.resize(msk,(256,256,1))\n",
        "  y.append(msk)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "# X_train=cv2.imread(\"/content/drive/MyDrive/Augmented/Images/0_0_4958.jpeg\")\n",
        "# X_train=np.resize(X_train,(1,256,256,3))\n",
        "# X_train=tf.expand_dims(X_train, axis=0)\n",
        "# y_train=cv2.imread(\"/content/drive/MyDrive/Augmented/Masks/0_0_4958.jpeg\")\n",
        "# y_train=np.resize(y_train,(1,256,256,1))\n",
        "# y_train=tf.expand_dims(y_train, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLozT-sMs9uM"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "# image_datagen = ImageDataGenerator(\n",
        "#                      rotation_range=40,\n",
        "#                      brightness_range=(0,5),zoom_range=0.2,\n",
        "#                      shear_range=0.2,\n",
        "#                      horizontal_flip=True,\n",
        "#                      vertical_flip=True,\n",
        "#                      fill_mode=\"nearest\",\n",
        "#                      ) # custom fuction for each image you can use resnet one too.\n",
        "\n",
        "# mask_datagen = ImageDataGenerator(\n",
        "#                      rotation_range=40,\n",
        "#                      brightness_range=(0,5),zoom_range=0.2,\n",
        "#                      shear_range=0.2,\n",
        "#                      horizontal_flip=True,\n",
        "#                      vertical_flip=True,\n",
        "#                      fill_mode=\"nearest\",\n",
        "#                      preprocessing_function=lambda x:np.where(x>0,1,0).astype()\n",
        "#                      ) # custom fuction for each image you can use resnet one too.\n",
        "\n",
        "# image_generator =image_datagen.flow_from_directory(\"/content/drive/MyDrive/Augmented/Images/\", class_mode=\"binary\",seed=42)\n",
        "# mask_generator = mask_datagen.flow_from_directory(\"/content/drive/MyDrive/Augmented/Masks/\", class_mode=\"binary\",seed=42)\n",
        "\n",
        "# train_generator = zip(image_generator, mask_generator)\n",
        "# train_generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X,y):\n",
        "  X=np.array(X)\n",
        "  y=np.array(y)\n",
        "  return X,y"
      ],
      "metadata": {
        "id": "jO0X4vyx1p41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X,y=preprocess(X,y)\n",
        "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.25,random_state=42)"
      ],
      "metadata": {
        "id": "3CdFj038rpnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ERROR CONVERT 5 DIMS TO 4 DIMS, EXTRA 1 DIM IS THERE"
      ],
      "metadata": {
        "id": "yeUmi-OR2DQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "seed=42\n",
        "image_data_gen_args=dict(rotation_range=90,\n",
        "                         zoom_range=0.3,\n",
        "                         horizontal_flip=True,\n",
        "                         vertical_flip=True,\n",
        "                         fill_mode=\"reflect\")\n",
        "\n",
        "mask_data_gen_args=dict(rotation_range=90,\n",
        "                         zoom_range=0.3,\n",
        "                         horizontal_flip=True,\n",
        "                         vertical_flip=True,\n",
        "                         fill_mode=\"reflect\",\n",
        "                         preprocessing_function=lambda x:np.where(x>0,1,0).astype(x.dtype))\n",
        "\n",
        "image_data_generator=ImageDataGenerator(**image_data_gen_args)\n",
        "image_data_generator.fit(X_train, augment=True, seed=seed)\n",
        "image_generator=image_data_generator.flow(X_train, seed=seed)\n",
        "valid_img_generator=image_data_generator.flow(X_test, seed=seed)\n",
        "\n",
        "mask_data_generator=ImageDataGenerator(**mask_data_gen_args)\n",
        "mask_data_generator.fit(y_train, seed=seed)\n",
        "mask_generator=mask_data_generator.flow(y_train, seed=seed)\n",
        "valid_mask_generator=mask_data_generator.flow(y_test, seed=seed)\n",
        "\n",
        "def my_image_mask_generator(image_generator, mask_generator):\n",
        "  train_generator=zip(image_generator, mask_generator)\n",
        "  for(img, mask) in train_generator:\n",
        "    yield (img, mask)\n",
        "\n",
        "my_generator = my_image_mask_generator(image_generator, mask_generator)\n",
        "validation_datagen = my_image_mask_generator(valid_img_generator, valid_mask_generator)"
      ],
      "metadata": {
        "id": "TZAaa2nsiLO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x=image_generator.next()\n",
        "y=mask_generator.next()\n",
        "for i in range(0,1):\n",
        "  image=x[i]\n",
        "  mask=y[i]\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(image[:,:,0],cmap=\"gray\")\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(mask[:,:,0])\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "HgD7XNbf3f5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVkmr4YGJdqY"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjdfGf5-JdqY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(inputs,num_filters):\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(inputs)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def encoder_block(inputs,num_filters):\n",
        "  x=conv_block(inputs,num_filters)\n",
        "  p=MaxPool2D((2,2))(x)\n",
        "  return x,p\n",
        "\n",
        "def decoder_block(inputs, skip_features,num_filters):\n",
        "  x=Conv2DTranspose(num_filters,(2,2),strides=2,padding=\"same\")(inputs)\n",
        "  x=Concatenate()([x,skip_features])\n",
        "  x=conv_block(x,num_filters)\n",
        "  return x\n",
        "\n",
        "def unet(input_shape):\n",
        "  inputs=Input(input_shape)\n",
        "\n",
        "  s1,p1=encoder_block(inputs,64)\n",
        "  s2,p2=encoder_block(p1,128)\n",
        "  s3,p3=encoder_block(p2,256)\n",
        "  s4,p4=encoder_block(p3,512)\n",
        "\n",
        "  b1 = conv_block(p4,1024)\n",
        "\n",
        "  d1 = decoder_block(b1,s4,512)\n",
        "  d2 = decoder_block(d1,s3,256)\n",
        "  d3 = decoder_block(d2,s2,128)\n",
        "  d4 = decoder_block(d3,s1,64)\n",
        "\n",
        "  outputs=Conv2D(1,(1,1),padding=\"same\",activation=\"sigmoid\")(d4)\n",
        "  model=Model(inputs,outputs,name=\"U-Net\")\n",
        "  return model  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8moIQEoJdqY"
      },
      "outputs": [],
      "source": [
        "# Model Summary\n",
        "model=unet((256,256,3))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0ca958-afa1-4691-df95-6ea8ca7f8029",
        "id": "yEykoegwJdqZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-4d1ec6ca2073>:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(my_generator, validation_data=validation_datagen, steps_per_epoch=32, validation_steps=50, epochs=100,callbacks=tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "32/32 [==============================] - 123s 3s/step - loss: 0.6664 - accuracy: 0.5969 - val_loss: 0.7375 - val_accuracy: 0.4983\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.6057 - accuracy: 0.6589 - val_loss: 0.6964 - val_accuracy: 0.5595\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.5779 - accuracy: 0.6851 - val_loss: 0.7027 - val_accuracy: 0.5719\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.5506 - accuracy: 0.7091 - val_loss: 0.6300 - val_accuracy: 0.6911\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.5418 - accuracy: 0.7250 - val_loss: 0.5739 - val_accuracy: 0.7347\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.5052 - accuracy: 0.7450 - val_loss: 0.7455 - val_accuracy: 0.6787\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.4945 - accuracy: 0.7552 - val_loss: 0.8358 - val_accuracy: 0.6686\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.4734 - accuracy: 0.7694 - val_loss: 0.7797 - val_accuracy: 0.6691\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 58s 2s/step - loss: 0.4612 - accuracy: 0.7798 - val_loss: 0.7222 - val_accuracy: 0.7107\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.4526 - accuracy: 0.7760 - val_loss: 0.7460 - val_accuracy: 0.7222\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.4315 - accuracy: 0.7927 - val_loss: 0.7783 - val_accuracy: 0.6925\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.4210 - accuracy: 0.8015 - val_loss: 0.9647 - val_accuracy: 0.6729\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.4060 - accuracy: 0.8144 - val_loss: 0.7825 - val_accuracy: 0.7578\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.3901 - accuracy: 0.8220 - val_loss: 0.7629 - val_accuracy: 0.7292\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.3825 - accuracy: 0.8224 - val_loss: 0.8038 - val_accuracy: 0.7510\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 60s 2s/step - loss: 0.3827 - accuracy: 0.8239 - val_loss: 1.1345 - val_accuracy: 0.6581\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.3597 - accuracy: 0.8354 - val_loss: 1.0045 - val_accuracy: 0.6700\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.3544 - accuracy: 0.8405 - val_loss: 0.7713 - val_accuracy: 0.7552\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.3520 - accuracy: 0.8411 - val_loss: 0.6287 - val_accuracy: 0.7869\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.3480 - accuracy: 0.8446 - val_loss: 0.8417 - val_accuracy: 0.7200\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.3426 - accuracy: 0.8493 - val_loss: 0.6735 - val_accuracy: 0.7591\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.3280 - accuracy: 0.8525 - val_loss: 0.5748 - val_accuracy: 0.8191\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.3111 - accuracy: 0.8634 - val_loss: 0.7452 - val_accuracy: 0.7435\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.3303 - accuracy: 0.8545 - val_loss: 0.7516 - val_accuracy: 0.7764\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.3068 - accuracy: 0.8651 - val_loss: 0.9313 - val_accuracy: 0.6968\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 60s 2s/step - loss: 0.3012 - accuracy: 0.8687 - val_loss: 0.7836 - val_accuracy: 0.7601\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.3019 - accuracy: 0.8692 - val_loss: 0.6667 - val_accuracy: 0.7647\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.3076 - accuracy: 0.8675 - val_loss: 0.5518 - val_accuracy: 0.7957\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2979 - accuracy: 0.8689 - val_loss: 0.6305 - val_accuracy: 0.8085\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.2869 - accuracy: 0.8752 - val_loss: 0.5771 - val_accuracy: 0.8082\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.3018 - accuracy: 0.8682 - val_loss: 0.7084 - val_accuracy: 0.8060\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2865 - accuracy: 0.8776 - val_loss: 0.6089 - val_accuracy: 0.7864\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2773 - accuracy: 0.8780 - val_loss: 0.6020 - val_accuracy: 0.7807\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2660 - accuracy: 0.8850 - val_loss: 0.7416 - val_accuracy: 0.7569\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2884 - accuracy: 0.8727 - val_loss: 0.8039 - val_accuracy: 0.7325\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2784 - accuracy: 0.8784 - val_loss: 0.7208 - val_accuracy: 0.7605\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.2682 - accuracy: 0.8834 - val_loss: 0.7375 - val_accuracy: 0.7251\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2679 - accuracy: 0.8850 - val_loss: 0.8573 - val_accuracy: 0.7282\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.2684 - accuracy: 0.8836 - val_loss: 0.8322 - val_accuracy: 0.7321\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2597 - accuracy: 0.8872 - val_loss: 0.6696 - val_accuracy: 0.7628\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2672 - accuracy: 0.8830 - val_loss: 0.9815 - val_accuracy: 0.7537\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2520 - accuracy: 0.8922 - val_loss: 0.9402 - val_accuracy: 0.7226\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2740 - accuracy: 0.8812 - val_loss: 0.8437 - val_accuracy: 0.7483\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2692 - accuracy: 0.8841 - val_loss: 0.6568 - val_accuracy: 0.7704\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2500 - accuracy: 0.8922 - val_loss: 0.5506 - val_accuracy: 0.8168\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2607 - accuracy: 0.8878 - val_loss: 0.7672 - val_accuracy: 0.7521\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.2577 - accuracy: 0.8872 - val_loss: 1.0181 - val_accuracy: 0.7216\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 60s 2s/step - loss: 0.2526 - accuracy: 0.8898 - val_loss: 0.5740 - val_accuracy: 0.7816\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.2451 - accuracy: 0.8941 - val_loss: 0.5928 - val_accuracy: 0.7870\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2634 - accuracy: 0.8861 - val_loss: 0.8125 - val_accuracy: 0.7477\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2427 - accuracy: 0.8949 - val_loss: 0.5929 - val_accuracy: 0.7819\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2474 - accuracy: 0.8915 - val_loss: 0.8168 - val_accuracy: 0.7397\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.2356 - accuracy: 0.8972 - val_loss: 0.6082 - val_accuracy: 0.7926\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2512 - accuracy: 0.8911 - val_loss: 0.7329 - val_accuracy: 0.7564\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2428 - accuracy: 0.8927 - val_loss: 0.8014 - val_accuracy: 0.7692\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2427 - accuracy: 0.8938 - val_loss: 0.6713 - val_accuracy: 0.7829\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.2345 - accuracy: 0.8964 - val_loss: 0.7540 - val_accuracy: 0.7780\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2419 - accuracy: 0.8947 - val_loss: 0.6505 - val_accuracy: 0.8196\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2370 - accuracy: 0.8958 - val_loss: 0.7233 - val_accuracy: 0.7729\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2339 - accuracy: 0.8968 - val_loss: 0.7296 - val_accuracy: 0.7485\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2393 - accuracy: 0.8948 - val_loss: 0.7068 - val_accuracy: 0.7692\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2331 - accuracy: 0.8977 - val_loss: 0.7638 - val_accuracy: 0.7688\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2408 - accuracy: 0.8958 - val_loss: 0.5247 - val_accuracy: 0.8176\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2299 - accuracy: 0.8991 - val_loss: 0.6017 - val_accuracy: 0.8004\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 61s 2s/step - loss: 0.2311 - accuracy: 0.8979 - val_loss: 0.7685 - val_accuracy: 0.7614\n",
            "Epoch 66/100\n",
            "29/32 [==========================>...] - ETA: 3s - loss: 0.2348 - accuracy: 0.8955"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "input_shape=(256,256,3)\n",
        "model=unet(input_shape)\n",
        "model.compile(tf.keras.optimizers.RMSprop(1e-4),loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "batch_size=64\n",
        "model.fit_generator(my_generator, validation_data=validation_datagen, steps_per_epoch=32, validation_steps=50, epochs=100,callbacks=tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\"))\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# model.compile()\n",
        "# model.compile(keras.optimizers.Adam(),loss=jaccard_loss,metrics=iou_metrics)\n",
        "# model.compile(loss=dice_loss, optimizer=Adam(1e-4),metrics=[dice_coef])\n",
        "# model.compile(loss=dice_loss,optimizer=Adam(),metrics=metrics)\n",
        "# metrics=[dice_coef,iou,Recall(),Precision()]\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# Callbacks\n",
        "# callbacks=[tf.keras.callbacks.EarlyStopping(patience=15,monitor=\"val_loss\"),\n",
        "#             tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\"),\n",
        "#             tf.keras.callbacks.ModelCheckpoint(\"model_for_balls.h5\",save_best_only=True)\n",
        "#             tf.keras.callbacks.ReduceLROnPlateau()]\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# model.fit()\n",
        "# results=model.fit(X_train,y_train,validation_split=0.1,batch_size=16,epochs=100,callbacks=callbacks)\n",
        "# model.fit(train_dataset,epochs=20,validation_data=valid_dataset,callbacks=callbacks)\n",
        "# val_generator=zip(X_val,y_val)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "#ReduceLROnPlateau[49:58]: https://www.youtube.com/watch?v=lstBIXVUoSM\n",
        "# model.save(\"/content/drive/MyDrive/Birds_Dataset_Main/Models/VGG19_Adam.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install segmentation_models\n",
        "import segmentation_models as sm\n",
        "jaccard_loss=sm.losses.bce_jaccard_loss\n",
        "iou_metrics=[sm.metrics.iou_score]"
      ],
      "metadata": {
        "id": "XuuthuMfohT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "test=cv2.imread(\"/content/Images/Images /3.jpeg\")\n",
        "original_shape=test.shape\n",
        "test=np.resize(test,(256,256,3))\n",
        "\n",
        "ground_truth=cv2.imread(\"/content/drive/MyDrive/Bitmaps/Masks /3.jpeg\")\n",
        "ground_truth=np.resize(ground_truth, (168, 300, 3))\n",
        "\n",
        "test_img_input=np.expand_dims(test,0)\n",
        "\n",
        "prediction=model.predict(test_img_input)\n",
        "prediction=(prediction>0.4)*255\n",
        "\n",
        "# prediction=np.array(prediction)\n",
        "prediction=np.resize(prediction, (168, 300, 1))\n",
        "prediction=prediction[0:,:,0]\n",
        "# prediction=np.resize(prediction,(177, 284,2))\n",
        "test=np.resize(test,(168, 300, 3))\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.subplot(231)\n",
        "plt.title(\"Testing image\")\n",
        "plt.imshow(test[:,:,0],cmap=\"gray\")\n",
        "plt.subplot(232)\n",
        "plt.title(\"Testing Label\")\n",
        "plt.imshow(ground_truth[:,:,0],cmap=\"gray\")\n",
        "plt.subplot(233)\n",
        "plt.title(\"Prediction\")\n",
        "plt.imshow(prediction,cmap=\"gray\")"
      ],
      "metadata": {
        "id": "cdZwaCDr4GlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI3hNGLW7NuT",
        "outputId": "c9f5f3af-f138-4238-b6da-2e50b7652d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(168, 300, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdSushUZTY_F"
      },
      "outputs": [],
      "source": [
        "def generator(images, masks):      \n",
        "  train_generator = zip(images, masks)\n",
        "  for (images, masks) in train_generator:\n",
        "    yield (images, masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSa0VC1OR7ft"
      },
      "outputs": [],
      "source": [
        "# def my_image_mask_generator(image_data_generator, mask_data_generator):\n",
        "#     train_generator = zip(image_data_generator, mask_data_generator)\n",
        "#     for (img, mask) in train_generator:\n",
        "#         yield (img, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI91SML2fjA2"
      },
      "outputs": [],
      "source": [
        "# train_generator=tf.data.Dataset.from_tensor_slices(X_train,y_train).batch(12, drop_remainder=True).repeat(count=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oslnDMb3Jdqa"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HiIcMnkJdqa"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/logs"
      ]
    }
  ]
}