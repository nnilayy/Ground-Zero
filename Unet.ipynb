{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqX3Kv2ez/LuiwWxSlMNQY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/Unet/blob/main/Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, Dropout\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "XwRi6kRLW2RM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(inputs,num_filters):\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(inputs)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def encoder_block(inputs,num_filters):\n",
        "  x=conv_block(inputs,num_filters)\n",
        "  p=MaxPool2D((2,2))(x)\n",
        "  return x,p\n",
        "\n",
        "def decoder_block(inputs, skip_features,num_filters):\n",
        "  x=Conv2DTranspose(num_filters,(2,2),strides=2,padding=\"same\")(inputs)\n",
        "  x=Concatenate()([x,skip_features])\n",
        "  x=conv_block(x,num_filters)\n",
        "  return x"
      ],
      "metadata": {
        "id": "5D6TluvZqSbZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unet(input_shape):\n",
        "  inputs=Input(input_shape)\n",
        "\n",
        "  s1,p1=encoder_block(inputs,64)\n",
        "  s2,p2=encoder_block(p1,128)\n",
        "  s3,p3=encoder_block(p2,256)\n",
        "  s4,p4=encoder_block(p3,512)\n",
        "\n",
        "  b1 = conv_block(p4,1024)\n",
        "\n",
        "  d1 = decoder_block(b1,s4,512)\n",
        "  d2 = decoder_block(d1,s3,256)\n",
        "  d3 = decoder_block(d2,s2,128)\n",
        "  d4 = decoder_block(d3,s1,64)\n",
        "\n",
        "  outputs=Conv2D(1,(1,1),padding=\"same\",activation=\"sigmoid\")(d4)\n",
        "  model=Model(inputs,outputs,name=\"U-Net\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "YlOkOO2msRC4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape=(96,96,3)\n",
        "model=build_unet(input_shape)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEjCI6pdtv71",
        "outputId": "442ca543-1cf4-4173-924c-e673f0cbf9d7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"U-Net\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)           [(None, 96, 96, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_100 (Conv2D)            (None, 96, 96, 256)  7168        ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_94 (BatchN  (None, 96, 96, 256)  1024       ['conv2d_100[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_94 (Activation)     (None, 96, 96, 256)  0           ['batch_normalization_94[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_101 (Conv2D)            (None, 96, 96, 256)  590080      ['activation_94[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_95 (BatchN  (None, 96, 96, 256)  1024       ['conv2d_101[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_95 (Activation)     (None, 96, 96, 256)  0           ['batch_normalization_95[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_22 (MaxPooling2D  (None, 48, 48, 256)  0          ['activation_95[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_102 (Conv2D)            (None, 48, 48, 512)  1180160     ['max_pooling2d_22[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_96 (BatchN  (None, 48, 48, 512)  2048       ['conv2d_102[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_96 (Activation)     (None, 48, 48, 512)  0           ['batch_normalization_96[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_103 (Conv2D)            (None, 48, 48, 512)  2359808     ['activation_96[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_97 (BatchN  (None, 48, 48, 512)  2048       ['conv2d_103[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_97 (Activation)     (None, 48, 48, 512)  0           ['batch_normalization_97[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_19 (Conv2DTra  (None, 96, 96, 256)  524544     ['activation_97[0][0]']          \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 96, 96, 512)  0           ['conv2d_transpose_19[0][0]',    \n",
            "                                                                  'activation_95[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_104 (Conv2D)            (None, 96, 96, 256)  1179904     ['concatenate_19[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_98 (BatchN  (None, 96, 96, 256)  1024       ['conv2d_104[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_98 (Activation)     (None, 96, 96, 256)  0           ['batch_normalization_98[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_105 (Conv2D)            (None, 96, 96, 256)  590080      ['activation_98[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_99 (BatchN  (None, 96, 96, 256)  1024       ['conv2d_105[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_99 (Activation)     (None, 96, 96, 256)  0           ['batch_normalization_99[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_106 (Conv2D)            (None, 96, 96, 1)    257         ['activation_99[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,440,193\n",
            "Trainable params: 6,436,097\n",
            "Non-trainable params: 4,096\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unet(pretrained_weights=None, input_size=(256,256,3)):\n",
        "  inputs=Input(input_size)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Encoder Block\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(inputs)\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv1)\n",
        "  pool1=MaxPool2D((2,2))(conv1)\n",
        "\n",
        "  conv2=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool1)\n",
        "  conv2=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv2)\n",
        "  pool2=MaxPool2D((2,2))(conv2)\n",
        "\n",
        "  conv3=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool2)\n",
        "  conv3=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv3)\n",
        "  pool3=MaxPool2D((2,2))(conv3)\n",
        "\n",
        "  conv4=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool3)\n",
        "  conv4=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv4)\n",
        "  pool4=MaxPool2D((2,2))(conv4)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Bottleneck\n",
        "  conv5=Conv2D(1024,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool4)\n",
        "  conv5=Conv2D(1024,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv5)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Decoder Block\n",
        "  up6=Conv2DTranspose(512,(2,2),strides=2,padding=\"same\")(conv5)\n",
        "  merge6=Concatenate()([up6,conv5],axis=3)\n",
        "  conv6=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge6)\n",
        "  conv6=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv6)\n",
        "\n",
        "  up7=Conv2DTranspose(256,(2,2),strides=2,padding=\"same\")(conv6)\n",
        "  merge7=Concatenate()([up7,conv3],axis=3)\n",
        "  conv7=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge7)\n",
        "  conv7=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv7)\n",
        "\n",
        "  up8=Conv2DTranspose(128,(2,2),strides=2,padding=\"same\")(conv7)\n",
        "  merge8=Concatenate()([up8,conv2],axis=3)\n",
        "  conv8=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge8)\n",
        "  conv8=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv8)\n",
        "\n",
        "  up9=Conv2DTranspose(64,(2,2),strides=2,padding=\"same\")(conv8)\n",
        "  merge9=Concatenate()([up8,conv2],axis=3)\n",
        "  conv9=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge9)\n",
        "  conv9=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv9)\n",
        "\n",
        "  conv10=Conv2D(1,1,activation=\"sigmoid\")(conv9)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "  model=Model(input=inputs, output=conv10)\n",
        "  model.compile(optimizer=Adam(lr=1e-4),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "  "
      ],
      "metadata": {
        "id": "-Kl5n1bex5RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You Can Add BatchNormalization Layers\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(inputs)\n",
        "  conv1=BatchNormalization()(conv1)\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv1)\n",
        "  conv1=BatchNormalization()(conv1)\n",
        "  pool1=MaxPool2D((2,2))(conv1)\n",
        "\n",
        "# You Can Add Dropout layers too\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(inputs)\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv1)\n",
        "  conv1=Dropout(0.2)(conv1) "
      ],
      "metadata": {
        "id": "ZtcrmBZt4Pp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u8Mbe098Afp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from akida_models import akidanet_imagenet\n",
        "from keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, Softmax, ReLU\n",
        "from cnn2snn import check_model_compatibility\n",
        "from ei_tensorflow.constrained_object_detection import models, dataset, metrics, util\n",
        "\n",
        "def build_model(input_shape: tuple, alpha: float,num_classes: int, weight_regularizer=None) -> tf.keras.Model:\n",
        "    \"\"\" Construct a constrained object detection model.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Passed to AkidaNet construction.\n",
        "        alpha: AkidaNet alpha value.\n",
        "        num_classes: Number of classes, i.e. final dimension size, in output.\n",
        "\n",
        "    Returns:\n",
        "        Uncompiled keras model.\n",
        "\n",
        "    Model takes (B, H, W, C) input and\n",
        "    returns (B, H//8, W//8, num_classes) logits.\n",
        "    \"\"\"\n",
        "    #! Create a quantized base model without top layers\n",
        "    a_base_model = akidanet_imagenet(input_shape=input_shape,alpha=alpha,include_top=False,input_scaling=None)\n",
        "    #! Get pretrained quantized weights and load them into the base model\n",
        "    #! Available base models are:\n",
        "    #! akidanet_imagenet_224_alpha_50.h5             - float32 model, 224x224x3, alpha=0.5\n",
        "    #! akidanet_imagenet_160_alpha_50.h5             - float32 model, 160x160x3, alpha=0.5\n",
        "    pretrained_weights = './transfer-learning-weights/akidanet/akidanet_imagenet_224_alpha_50.h5'\n",
        "    a_base_model.load_weights(pretrained_weights, by_name=True, skip_mismatch=True)\n",
        "    a_base_model.trainable = True\n",
        "    #! Default batch norm is configured for huge networks, let's speed it up\n",
        "    for layer in a_base_model.layers:\n",
        "        if type(layer) == BatchNormalization:\n",
        "            layer.momentum = 0.9\n",
        "    #! Cut AkidaNet where it hits 1/8th input resolution; i.e. (HW/8, HW/8, C)\n",
        "    a_cut_point = a_base_model.get_layer('separable_5_relu')\n",
        "    #! Now attach a small additional head on the AkidaNet\n",
        "    a_model_part_head = Conv2D(filters=32, kernel_size=1, strides=1, padding='same',kernel_regularizer=weight_regularizer)(a_cut_point.output)\n",
        "    a_model_part = ReLU()(a_model_part_head)\n",
        "    a_logits = Conv2D(filters=num_classes, kernel_size=1, strides=1, padding='same',activation=None, kernel_regularizer=weight_regularizer)(a_model_part)\n",
        "    fomo_akida = Model(inputs=a_base_model.input, outputs=a_logits)\n",
        "    #! Check if the model is sompatbile with Akida (fail quickly before training)\n",
        "    compatible = check_model_compatibility(fomo_akida, input_is_image=True)\n",
        "    if not compatible:\n",
        "        print(\"Model is not compatible with Akida!\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    return fomo_akida\n",
        "\n",
        "def train(num_classes: int, learning_rate: float, num_epochs: int,alpha: float, object_weight: int,train_dataset: tf.data.Dataset,validation_dataset: tf.data.Dataset,best_model_path: str,input_shape: tuple, callbacks: 'list',quantize_function,lr_finder: bool = False) -> tf.keras.Model:\n",
        "    \"\"\" Construct and train a constrained object detection model.\n",
        "\n",
        "    Args:\n",
        "        num_classes: Number of classes in datasets. This does not include\n",
        "        implied background class introduced by segmentation map dataset\n",
        "        conversion.\n",
        "        learning_rate: Learning rate for Adam.\n",
        "        num_epochs: Number of epochs passed to model.fit\n",
        "        alpha: Alpha used to construct AkidaNet. Pretrained weights will be\n",
        "        used if there is a matching set.\n",
        "        object_weight: The weighting to give the object in the loss function\n",
        "            where background has an implied weight of 1.0.\n",
        "        train_dataset: Training dataset of (x, (bbox, one_hot_y))\n",
        "        validation_dataset: Validation dataset of (x, (bbox, one_hot_y))\n",
        "        best_model_path: location to save best model path. note: weights\n",
        "            will be restored from this path based on best val_f1 score.\n",
        "        input_shape: The shape of the model's input\n",
        "        lr_finder: TODO\n",
        "    Returns:\n",
        "        Trained keras model.\n",
        "\n",
        "    Constructs a new constrained object detection model with num_classes+1\n",
        "    outputs (denoting the classes with an implied background class of 0).\n",
        "    Both training and validation datasets are adapted from\n",
        "    (x, (bbox, one_hot_y)) to (x, segmentation_map). Model is trained with a\n",
        "    custom weighted cross entropy function.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    num_classes_with_background = num_classes + 1\n",
        "\n",
        "    input_width_height = None\n",
        "    width, height, input_num_channels = input_shape\n",
        "    if width != height:\n",
        "        raise Exception(f\"Only square inputs are supported; not {input_shape}\")\n",
        "    input_width_height = width\n",
        "\n",
        "    model = build_model(input_shape=input_shape,alpha=alpha,num_classes=num_classes_with_background,weight_regularizer=tf.keras.regularizers.l2(4e-5))\n",
        "    #! Derive output size from model\n",
        "    model_output_shape = model.layers[-1].output.shape\n",
        "    _batch, width, height, num_classes = model_output_shape\n",
        "    if width != height:\n",
        "        raise Exception(f\"Only square outputs are supported; not {model_output_shape}\")\n",
        "    output_width_height = width\n",
        "\n",
        "    #! Build weighted cross entropy loss specific to this model size\n",
        "    weighted_xent = models.construct_weighted_xent_fn(model.output.shape, object_weight)\n",
        "    #! Transform bounding box labels into segmentation maps\n",
        "    train_segmentation_dataset = train_dataset.map(dataset.bbox_to_segmentation(output_width_height, num_classes_with_background)).batch(32, drop_remainder=False).prefetch(1)\n",
        "    validation_segmentation_dataset = validation_dataset.map(dataset.bbox_to_segmentation(output_width_height, num_classes_with_background, validation=True)).batch(32, drop_remainder=False).prefetch(1)\n",
        "    #! Initialise bias of final classifier based on training data prior.\n",
        "    util.set_classifier_biases_from_dataset(model, train_segmentation_dataset)\n",
        "    if lr_finder:\n",
        "        learning_rate = ei_tensorflow.lr_finder.find_lr(model, train_segmentation_dataset, weighted_xent)\n",
        "\n",
        "    opt = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss=weighted_xent,optimizer=opt)\n",
        "\n",
        "    #! Create callback that will do centroid scoring on end of epoch against\n",
        "    #! validation data. Include a callback to show % progress in slow cases.\n",
        "    centroid_callback = metrics.CentroidScoring(validation_segmentation_dataset,output_width_height, num_classes_with_background)\n",
        "    print_callback = metrics.PrintPercentageTrained(num_epochs)\n",
        "\n",
        "    #! Include a callback for model checkpointing based on the best validation f1.\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(best_model_path,monitor='val_f1', save_best_only=True, mode='max',save_weights_only=True, verbose=0)\n",
        "    model.fit(train_segmentation_dataset,validation_data=validation_segmentation_dataset,epochs=num_epochs,callbacks=callbacks + [centroid_callback, print_callback, checkpoint_callback],verbose=0)\n",
        "    #! Restore best weights.\n",
        "    model.load_weights(best_model_path)\n",
        "    #! Add explicit softmax layer before export.\n",
        "    softmax_layer = Softmax()(model.layers[-1].output)\n",
        "    model = Model(model.input, softmax_layer)\n",
        "    #! Check if model is compatible with Akida\n",
        "    compatible = check_model_compatibility(model, input_is_image=True)\n",
        "    if not compatible:\n",
        "        print(\"Model is not compatible with Akida!\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    akida_model = quantize_function(model=model,train_dataset=train_segmentation_dataset,validation_dataset=validation_segmentation_dataset,optimizer=opt,fine_tune_loss=weighted_xent,fine_tune_metrics=None,best_model_path=best_model_path,callbacks=callbacks + [centroid_callback, print_callback],stopping_metric='val_f1',verbose=0)\n",
        "\n",
        "    return model, akida_model\n",
        "\n",
        "\n",
        "EPOCHS = args.epochs or 100\n",
        "LEARNING_RATE = args.learning_rate or 0.001\n",
        "\n",
        "def quantize_brainchip(model,train_dataset: tf.data.Dataset,validation_dataset: tf.data.Dataset,best_model_path: str, optimizer: str,fine_tune_loss: str,fine_tune_metrics: 'list[str]',callbacks, stopping_metric='val_accuracy',verbose=2):\n",
        "    import tensorflow as tf\n",
        "    import cnn2snn\n",
        "\n",
        "    print('Performing post-training quantization...')\n",
        "    akida_model = cnn2snn.quantize(model,weight_quantization=4,activ_quantization=4,input_weight_quantization=8)\n",
        "    print('Performing post-training quantization OK')\n",
        "    print('')\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=stopping_metric,mode='max',verbose=1,min_delta=0,patience=10,restore_best_weights=True)\n",
        "    callbacks.append(early_stopping)\n",
        "\n",
        "    print('Running quantization-aware training...')\n",
        "    akida_model.compile(optimizer=optimizer,loss=fine_tune_loss,metrics=fine_tune_metrics)\n",
        "    akida_model.fit(train_dataset,epochs=30,verbose=verbose,validation_data=validation_dataset,callbacks=callbacks)\n",
        "    print('Running quantization-aware training OK')\n",
        "    print('')\n",
        "\n",
        "    return akida_model\n",
        "\n",
        "\n",
        "model, akida_model = train(num_classes=classes,learning_rate=LEARNING_RATE,num_epochs=EPOCHS,alpha=0.5,object_weight=100,train_dataset=train_dataset,validation_dataset=validation_dataset,best_model_path=BEST_MODEL_PATH,input_shape=MODEL_INPUT_SHAPE,callbacks=callbacks,quantize_function=quantize_brainchip,lr_finder=False)\n",
        "override_mode = 'segmentation'\n",
        "disable_per_channel_quantization = False"
      ]
    }
  ]
}