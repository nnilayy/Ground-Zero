{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "R6DXuEhE4GYS",
        "ODkd18rA2n4S",
        "lXP3i2-3_iuB"
      ],
      "authorship_tag": "ABX9TyOUOVdJ0R16VjsiBvhP4Hve",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/Unet/blob/main/Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "esJE2uN6zlsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "img=read_image(\"/content/Images/1.jpeg\")\n",
        "cv2_imshow(img*255)\n",
        "img.shape"
      ],
      "metadata": {
        "id": "IIcTGWjyk2la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Preprocessing"
      ],
      "metadata": {
        "id": "R6DXuEhE4GYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob \n",
        "import numpy as np\n",
        "X_train=[]\n",
        "y_train=[]\n",
        "images=sorted(glob.glob(\"/content/drive/MyDrive/Augmented/Images/*\"))\n",
        "masks=sorted(glob.glob(\"/content/drive/MyDrive/Augmented/Masks/*\"))\n",
        "\n",
        "\n",
        "for image,mask in zip(images,masks):\n",
        "  img=cv2.imread(image)\n",
        "  img=np.array(img)\n",
        "  img=np.resize(img,(1,256,256,3))\n",
        "  X_train.append(img)\n",
        " \n",
        "  msk=cv2.imread(mask)\n",
        "  msk=np.array(msk)\n",
        "  msk=np.resize(msk,(1,256,256,1))\n",
        "  y_train.append(msk)\n",
        "\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "\n",
        "# X_train=cv2.imread(\"/content/drive/MyDrive/Augmented/Images/0_0_4958.jpeg\")\n",
        "# X_train=np.resize(X_train,(1,256,256,3))\n",
        "# X_train=tf.expand_dims(X_train, axis=0)\n",
        "# y_train=cv2.imread(\"/content/drive/MyDrive/Augmented/Masks/0_0_4958.jpeg\")\n",
        "# y_train=np.resize(y_train,(1,256,256,1))\n",
        "# y_train=tf.expand_dims(y_train, axis=0)\n"
      ],
      "metadata": {
        "id": "ZChbB-ovF13Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "ODkd18rA2n4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(inputs,num_filters):\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(inputs)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def encoder_block(inputs,num_filters):\n",
        "  x=conv_block(inputs,num_filters)\n",
        "  p=MaxPool2D((2,2))(x)\n",
        "  return x,p\n",
        "\n",
        "def decoder_block(inputs, skip_features,num_filters):\n",
        "  x=Conv2DTranspose(num_filters,(2,2),strides=2,padding=\"same\")(inputs)\n",
        "  x=Concatenate()([x,skip_features])\n",
        "  x=conv_block(x,num_filters)\n",
        "  return x\n",
        "\n",
        "def unet(input_shape):\n",
        "  inputs=Input(input_shape)\n",
        "\n",
        "  s1,p1=encoder_block(inputs,64)\n",
        "  s2,p2=encoder_block(p1,128)\n",
        "  s3,p3=encoder_block(p2,256)\n",
        "  s4,p4=encoder_block(p3,512)\n",
        "\n",
        "  b1 = conv_block(p4,1024)\n",
        "\n",
        "  d1 = decoder_block(b1,s4,512)\n",
        "  d2 = decoder_block(d1,s3,256)\n",
        "  d3 = decoder_block(d2,s2,128)\n",
        "  d4 = decoder_block(d3,s1,64)\n",
        "\n",
        "  outputs=Conv2D(1,(1,1),padding=\"same\",activation=\"sigmoid\")(d4)\n",
        "  model=Model(inputs,outputs,name=\"U-Net\")\n",
        "  return model  "
      ],
      "metadata": {
        "id": "5D6TluvZqSbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Summary\n",
        "model=unet((256,256,3))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "8PkqYSaC6gdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# Model\n",
        "input_shape=(256,256,3)\n",
        "model=unet(input_shape)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# model.compile()\n",
        "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "# model.compile(loss=dice_loss, optimizer=Adam(1e-4),metrics=[dice_coef])\n",
        "# model.compile(loss=dice_loss,optimizer=Adam(),metrics=metrics)\n",
        "# metrics=[dice_coef,iou,Recall(),Precision()]\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# Callbacks\n",
        "# callbacks=[tf.keras.callbacks.EarlyStopping(patience=15,monitor=\"val_loss\"),\n",
        "#             tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\"),\n",
        "#             tf.keras.callbacks.ModelCheckpoint(\"model_for_balls.h5\",save_best_only=True)\n",
        "#             tf.keras.callbacks.ReduceLROnPlateau()]\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# model.fit()\n",
        "# results=model.fit(X_train,y_train,validation_split=0.1,batch_size=16,epochs=100,callbacks=callbacks)\n",
        "# model.fit(train_dataset,epochs=20,validation_data=valid_dataset,callbacks=callbacks)\n",
        "train_generator=zip(X_train,y_train)\n",
        "val_generator=zip(X_val,y_val)\n",
        "batch_size=16\n",
        "model.fit(train_generator,steps_per_epoch=60//batch_size,batch_size=batch_size, epochs=10,callbacks=tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\"))\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "#ReduceLROnPlateau[49:58]: https://www.youtube.com/watch?v=lstBIXVUoSM\n",
        "# model.save(\"/content/drive/MyDrive/Birds_Dataset_Main/Models/VGG19_Adam.h5\")"
      ],
      "metadata": {
        "id": "nzjXl5p7fSbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard"
      ],
      "metadata": {
        "id": "lXP3i2-3_iuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/logs"
      ],
      "metadata": {
        "id": "f3X8lq1ANP5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Models"
      ],
      "metadata": {
        "id": "I_DAWWLrimNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def unet(pretrained_weights=None, input_size=(256,256,3)):\n",
        "  inputs=Input(input_size)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Encoder Block\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(inputs)\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv1)\n",
        "  pool1=MaxPool2D((2,2))(conv1)\n",
        "\n",
        "  conv2=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool1)\n",
        "  conv2=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv2)\n",
        "  pool2=MaxPool2D((2,2))(conv2)\n",
        "\n",
        "  conv3=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool2)\n",
        "  conv3=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv3)\n",
        "  pool3=MaxPool2D((2,2))(conv3)\n",
        "\n",
        "  conv4=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool3)\n",
        "  conv4=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv4)\n",
        "  pool4=MaxPool2D((2,2))(conv4)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Bottleneck\n",
        "  conv5=Conv2D(1024,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(pool4)\n",
        "  conv5=Conv2D(1024,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv5)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Decoder Block\n",
        "  up6=Conv2DTranspose(512,(2,2),strides=2,padding=\"same\")(conv5)\n",
        "  merge6=Concatenate()([up6,conv5],axis=3)\n",
        "  conv6=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge6)\n",
        "  conv6=Conv2D(512,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv6)\n",
        "\n",
        "  up7=Conv2DTranspose(256,(2,2),strides=2,padding=\"same\")(conv6)\n",
        "  merge7=Concatenate()([up7,conv3],axis=3)\n",
        "  conv7=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge7)\n",
        "  conv7=Conv2D(256,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv7)\n",
        "\n",
        "  up8=Conv2DTranspose(128,(2,2),strides=2,padding=\"same\")(conv7)\n",
        "  merge8=Concatenate()([up8,conv2],axis=3)\n",
        "  conv8=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge8)\n",
        "  conv8=Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv8)\n",
        "\n",
        "  up9=Conv2DTranspose(64,(2,2),strides=2,padding=\"same\")(conv8)\n",
        "  merge9=Concatenate()([up8,conv2],axis=3)\n",
        "  conv9=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(merge9)\n",
        "  conv9=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv9)\n",
        "\n",
        "  conv10=Conv2D(1,1,activation=\"sigmoid\")(conv9)\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "  model=Model(input=inputs, output=conv10)\n",
        "  model.compile(optimizer=Adam(lr=1e-4),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "-Kl5n1bex5RS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# You Can Add BatchNormalization Layers\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(inputs)\n",
        "  conv1=BatchNormalization()(conv1)\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv1)\n",
        "  conv1=BatchNormalization()(conv1)\n",
        "  pool1=MaxPool2D((2,2))(conv1)\n",
        "\n",
        "# You Can Add Dropout layers too\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(inputs)\n",
        "  conv1=Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(conv1)\n",
        "  conv1=Dropout(0.2)(conv1) "
      ],
      "metadata": {
        "id": "ZtcrmBZt4Pp2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import tensorflow as tf \n",
        "IMG_WIDTH=128\n",
        "IMG_HEIGHT=128\n",
        "IMG_CHANNEL=3\n",
        "\n",
        "inputs=tf.keras.layers.Input((IMG_WIDTH,IMG_HEIGHT,IMG_CHANNEL))\n",
        "s=tf.keras.layers.Lambda(lambda x:x/255)(inputs)\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# CONTRACTION PATH \n",
        "# --------------------------------------------------------------------------------------------\n",
        "c1=tf.keras.layers.Conv2D(16,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(inputs)\n",
        "c1=tf.keras.layers.Dropout(0.1)(c1)\n",
        "c1=tf.keras.layers.Conv2D(16,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(c1)\n",
        "p1=tf.keras.layers.MaxPooling2D((2,2))(c1)\n",
        "\n",
        "c2=tf.keras.layers.Conv2D(32,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(p1)\n",
        "c2=tf.keras.layers.Dropout(0.1)(c2)\n",
        "c2=tf.keras.layers.Conv2D(32,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(c2)\n",
        "p2=tf.keras.layers.MaxPooling2D((2,2))(c2)\n",
        "\n",
        "c3=tf.keras.layers.Conv2D(64,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(p2)\n",
        "c3=tf.keras.layers.Dropout(0.1)(c3)\n",
        "c3=tf.keras.layers.Conv2D(64,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(c3)\n",
        "p3=tf.keras.layers.MaxPooling2D((2,2))(c3)\n",
        "\n",
        "c4=tf.keras.layers.Conv2D(128,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(p3)\n",
        "c4=tf.keras.layers.Dropout(0.1)(c4)\n",
        "c4=tf.keras.layers.Conv2D(128,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(c4)\n",
        "p4=tf.keras.layers.MaxPooling2D((2,2))(c4)\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# BOTTLENECK\n",
        "# --------------------------------------------------------------------------------------------\n",
        "c5=tf.keras.layers.Conv2D(256,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(p4)\n",
        "c5=tf.keras.layers.Dropout(0.1)(c5)\n",
        "c5=tf.keras.layers.Conv2D(256,(3,3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\")(c5)\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# EXPANSION PATH\n",
        "# --------------------------------------------------------------------------------------------\n",
        "u6=tf.keras.layers.Conv2DTranspose(128,(2,2),strides=2,padding=\"same\")(c5)\n",
        "u6=tf.keras.layers.concatenate([u6,c4])  \n",
        "c6=tf.keras.layers.Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(u6)\n",
        "c6=tf.keras.layers.Conv2D(128,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(c6)\n",
        "\n",
        "u7=tf.keras.layers.Conv2DTranspose(64,(2,2),strides=2,padding=\"same\")(c6)\n",
        "u7=tf.keras.layers.concatenate([u7,c3])\n",
        "c7=tf.keras.layers.Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(u7)\n",
        "c7=tf.keras.layers.Conv2D(64,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(c7)\n",
        "\n",
        "u8=tf.keras.layers.Conv2DTranspose(23,(2,2),strides=2,padding=\"same\")(c7)\n",
        "u8=tf.keras.layers.concatenate([u8,c2])\n",
        "c8=tf.keras.layers.Conv2D(32,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(u8)\n",
        "c8=tf.keras.layers.Conv2D(32,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(c8)\n",
        "\n",
        "u9=tf.keras.layers.Conv2DTranspose(16,(2,2),strides=2,padding=\"same\")(c8)\n",
        "u9=tf.keras.layers.concatenate([u9,c1])\n",
        "c9=tf.keras.layers.Conv2D(16,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(u9)\n",
        "c9=tf.keras.layers.Conv2D(16,3,activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(c9)\n",
        "\n",
        "outputs=tf.keras.layers.Conv2D(1,1,activation=\"sigmoid\")(c9)\n",
        "\n",
        "model=tf.keras.Model(inputs=[inputs],outputs=[outputs])\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "z2bMxmgogTw9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Getting Dataset flow ready\n",
        "# ()Build a Model\n",
        "# ()Test if it works \n",
        "# ()Can it be deployed on Spresense \n",
        "# ()If so what are the results"
      ],
      "metadata": {
        "id": "ZKw192PJpErR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Removing and Adding Filter layers reduces params\n",
        "# Train the model\n",
        "# What is size of the weight file in the end\n",
        "# What is weight file after converting to tflite file\n",
        "# Can it run on Spresense \n",
        "# Can it be quantized further\n",
        "\n",
        "# 11:00-78"
      ],
      "metadata": {
        "id": "Ns6RJP_BpGBR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# ()Data analysis Project Repo\n",
        "# ()Read Me for Different projects\n",
        "# ()Movie Recommendation System"
      ],
      "metadata": {
        "id": "w8BrvFEKoqOR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u8Mbe098Afp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from akida_models import akidanet_imagenet\n",
        "from keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, Softmax, ReLU\n",
        "from cnn2snn import check_model_compatibility\n",
        "from ei_tensorflow.constrained_object_detection import models, dataset, metrics, util\n",
        "\n",
        "def build_model(input_shape: tuple, alpha: float,num_classes: int, weight_regularizer=None) -> tf.keras.Model:\n",
        "    \"\"\" Construct a constrained object detection model.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Passed to AkidaNet construction.\n",
        "        alpha: AkidaNet alpha value.\n",
        "        num_classes: Number of classes, i.e. final dimension size, in output.\n",
        "\n",
        "    Returns:\n",
        "        Uncompiled keras model.\n",
        "\n",
        "    Model takes (B, H, W, C) input and\n",
        "    returns (B, H//8, W//8, num_classes) logits.\n",
        "    \"\"\"\n",
        "    #! Create a quantized base model without top layers\n",
        "    a_base_model = akidanet_imagenet(input_shape=input_shape,alpha=alpha,include_top=False,input_scaling=None)\n",
        "    #! Get pretrained quantized weights and load them into the base model\n",
        "    #! Available base models are:\n",
        "    #! akidanet_imagenet_224_alpha_50.h5             - float32 model, 224x224x3, alpha=0.5\n",
        "    #! akidanet_imagenet_160_alpha_50.h5             - float32 model, 160x160x3, alpha=0.5\n",
        "    pretrained_weights = './transfer-learning-weights/akidanet/akidanet_imagenet_224_alpha_50.h5'\n",
        "    a_base_model.load_weights(pretrained_weights, by_name=True, skip_mismatch=True)\n",
        "    a_base_model.trainable = True\n",
        "    #! Default batch norm is configured for huge networks, let's speed it up\n",
        "    for layer in a_base_model.layers:\n",
        "        if type(layer) == BatchNormalization:\n",
        "            layer.momentum = 0.9\n",
        "    #! Cut AkidaNet where it hits 1/8th input resolution; i.e. (HW/8, HW/8, C)\n",
        "    a_cut_point = a_base_model.get_layer('separable_5_relu')\n",
        "    #! Now attach a small additional head on the AkidaNet\n",
        "    a_model_part_head = Conv2D(filters=32, kernel_size=1, strides=1, padding='same',kernel_regularizer=weight_regularizer)(a_cut_point.output)\n",
        "    a_model_part = ReLU()(a_model_part_head)\n",
        "    a_logits = Conv2D(filters=num_classes, kernel_size=1, strides=1, padding='same',activation=None, kernel_regularizer=weight_regularizer)(a_model_part)\n",
        "    fomo_akida = Model(inputs=a_base_model.input, outputs=a_logits)\n",
        "    #! Check if the model is sompatbile with Akida (fail quickly before training)\n",
        "    compatible = check_model_compatibility(fomo_akida, input_is_image=True)\n",
        "    if not compatible:\n",
        "        print(\"Model is not compatible with Akida!\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    return fomo_akida\n",
        "\n",
        "def train(num_classes: int, learning_rate: float, num_epochs: int,alpha: float, object_weight: int,train_dataset: tf.data.Dataset,validation_dataset: tf.data.Dataset,best_model_path: str,input_shape: tuple, callbacks: 'list',quantize_function,lr_finder: bool = False) -> tf.keras.Model:\n",
        "    \"\"\" Construct and train a constrained object detection model.\n",
        "\n",
        "    Args:\n",
        "        num_classes: Number of classes in datasets. This does not include\n",
        "        implied background class introduced by segmentation map dataset\n",
        "        conversion.\n",
        "        learning_rate: Learning rate for Adam.\n",
        "        num_epochs: Number of epochs passed to model.fit\n",
        "        alpha: Alpha used to construct AkidaNet. Pretrained weights will be\n",
        "        used if there is a matching set.\n",
        "        object_weight: The weighting to give the object in the loss function\n",
        "            where background has an implied weight of 1.0.\n",
        "        train_dataset: Training dataset of (x, (bbox, one_hot_y))\n",
        "        validation_dataset: Validation dataset of (x, (bbox, one_hot_y))\n",
        "        best_model_path: location to save best model path. note: weights\n",
        "            will be restored from this path based on best val_f1 score.\n",
        "        input_shape: The shape of the model's input\n",
        "        lr_finder: TODO\n",
        "    Returns:\n",
        "        Trained keras model.\n",
        "\n",
        "    Constructs a new constrained object detection model with num_classes+1\n",
        "    outputs (denoting the classes with an implied background class of 0).\n",
        "    Both training and validation datasets are adapted from\n",
        "    (x, (bbox, one_hot_y)) to (x, segmentation_map). Model is trained with a\n",
        "    custom weighted cross entropy function.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    num_classes_with_background = num_classes + 1\n",
        "\n",
        "    input_width_height = None\n",
        "    width, height, input_num_channels = input_shape\n",
        "    if width != height:\n",
        "        raise Exception(f\"Only square inputs are supported; not {input_shape}\")\n",
        "    input_width_height = width\n",
        "\n",
        "    model = build_model(input_shape=input_shape,alpha=alpha,num_classes=num_classes_with_background,weight_regularizer=tf.keras.regularizers.l2(4e-5))\n",
        "    #! Derive output size from model\n",
        "    model_output_shape = model.layers[-1].output.shape\n",
        "    _batch, width, height, num_classes = model_output_shape\n",
        "    if width != height:\n",
        "        raise Exception(f\"Only square outputs are supported; not {model_output_shape}\")\n",
        "    output_width_height = width\n",
        "\n",
        "    #! Build weighted cross entropy loss specific to this model size\n",
        "    weighted_xent = models.construct_weighted_xent_fn(model.output.shape, object_weight)\n",
        "    #! Transform bounding box labels into segmentation maps\n",
        "    train_segmentation_dataset = train_dataset.map(dataset.bbox_to_segmentation(output_width_height, num_classes_with_background)).batch(32, drop_remainder=False).prefetch(1)\n",
        "    validation_segmentation_dataset = validation_dataset.map(dataset.bbox_to_segmentation(output_width_height, num_classes_with_background, validation=True)).batch(32, drop_remainder=False).prefetch(1)\n",
        "    #! Initialise bias of final classifier based on training data prior.\n",
        "    util.set_classifier_biases_from_dataset(model, train_segmentation_dataset)\n",
        "    if lr_finder:\n",
        "        learning_rate = ei_tensorflow.lr_finder.find_lr(model, train_segmentation_dataset, weighted_xent)\n",
        "\n",
        "    opt = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss=weighted_xent,optimizer=opt)\n",
        "\n",
        "    #! Create callback that will do centroid scoring on end of epoch against\n",
        "    #! validation data. Include a callback to show % progress in slow cases.\n",
        "    centroid_callback = metrics.CentroidScoring(validation_segmentation_dataset,output_width_height, num_classes_with_background)\n",
        "    print_callback = metrics.PrintPercentageTrained(num_epochs)\n",
        "\n",
        "    #! Include a callback for model checkpointing based on the best validation f1.\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(best_model_path,monitor='val_f1', save_best_only=True, mode='max',save_weights_only=True, verbose=0)\n",
        "    model.fit(train_segmentation_dataset,validation_data=validation_segmentation_dataset,epochs=num_epochs,callbacks=callbacks + [centroid_callback, print_callback, checkpoint_callback],verbose=0)\n",
        "    #! Restore best weights.\n",
        "    model.load_weights(best_model_path)\n",
        "    #! Add explicit softmax layer before export.\n",
        "    softmax_layer = Softmax()(model.layers[-1].output)\n",
        "    model = Model(model.input, softmax_layer)\n",
        "    #! Check if model is compatible with Akida\n",
        "    compatible = check_model_compatibility(model, input_is_image=True)\n",
        "    if not compatible:\n",
        "        print(\"Model is not compatible with Akida!\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    akida_model = quantize_function(model=model,train_dataset=train_segmentation_dataset,validation_dataset=validation_segmentation_dataset,optimizer=opt,fine_tune_loss=weighted_xent,fine_tune_metrics=None,best_model_path=best_model_path,callbacks=callbacks + [centroid_callback, print_callback],stopping_metric='val_f1',verbose=0)\n",
        "\n",
        "    return model, akida_model\n",
        "\n",
        "\n",
        "EPOCHS = args.epochs or 100\n",
        "LEARNING_RATE = args.learning_rate or 0.001\n",
        "\n",
        "def quantize_brainchip(model,train_dataset: tf.data.Dataset,validation_dataset: tf.data.Dataset,best_model_path: str, optimizer: str,fine_tune_loss: str,fine_tune_metrics: 'list[str]',callbacks, stopping_metric='val_accuracy',verbose=2):\n",
        "    import tensorflow as tf\n",
        "    import cnn2snn\n",
        "\n",
        "    print('Performing post-training quantization...')\n",
        "    akida_model = cnn2snn.quantize(model,weight_quantization=4,activ_quantization=4,input_weight_quantization=8)\n",
        "    print('Performing post-training quantization OK')\n",
        "    print('')\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=stopping_metric,mode='max',verbose=1,min_delta=0,patience=10,restore_best_weights=True)\n",
        "    callbacks.append(early_stopping)\n",
        "\n",
        "    print('Running quantization-aware training...')\n",
        "    akida_model.compile(optimizer=optimizer,loss=fine_tune_loss,metrics=fine_tune_metrics)\n",
        "    akida_model.fit(train_dataset,epochs=30,verbose=verbose,validation_data=validation_dataset,callbacks=callbacks)\n",
        "    print('Running quantization-aware training OK')\n",
        "    print('')\n",
        "\n",
        "    return akida_model\n",
        "\n",
        "\n",
        "model, akida_model = train(num_classes=classes,learning_rate=LEARNING_RATE,num_epochs=EPOCHS,alpha=0.5,object_weight=100,train_dataset=train_dataset,validation_dataset=validation_dataset,best_model_path=BEST_MODEL_PATH,input_shape=MODEL_INPUT_SHAPE,callbacks=callbacks,quantize_function=quantize_brainchip,lr_finder=False)\n",
        "override_mode = 'segmentation'\n",
        "disable_per_channel_quantization = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Based UNET\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KwpvzoaMYz_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import backend as K\n",
        "import cv2\n",
        "H=256\n",
        "W=256\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# iou()\n",
        "def iou(y_true,y_pred):\n",
        "  def f(y_true,y_pred):\n",
        "    intersection=(y_true * y_pred).sum()\n",
        "    union=y_true.sum()+y_pred.sum()-intersection\n",
        "    x=(intersection+1e-15)/(union+1e-15)\n",
        "    x=x.astype(np.float32)\n",
        "    return x\n",
        "  return tf.numpy_function(f,[y_true,y_pred],tf.float32)\n",
        "\n",
        "# dice_coef()\n",
        "smooth=1e-15\n",
        "def dice_coef(y_true,y_pred):\n",
        "  y_true=tf.keras.layers.Flatten()(y_true)\n",
        "  y_pred=tf.keras.layers.Flatten()(y_pred)\n",
        "  intersection=tf.reduce_sum(y_true*y_pred)\n",
        "  return (2. * intersection+smooth)/(tf.reduce_sum(y_true)+tf.reduce_sum(y_pred)+smooth)\n",
        "\n",
        "# dice_loss()\n",
        "def dice_loss(y_true,y_pred):\n",
        "  return 1.0-dice_coef(y_true,y_pred)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# Shuffling()\n",
        "def shuffling(x,y):\n",
        "  x,y=shuffle(x,y,random_state=42)\n",
        "  return x,y\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# load_data()\n",
        "def load_data(path):\n",
        "  x=sorted(glob.glob())\n",
        "  y=sorted(glob.glob())\n",
        "  return x,y\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# X_train, y_train=load_data(train_path)\n",
        "# X_train, y_train=shuffling(X_train, y_train)\n",
        "# X_test, y_test=load_data(test_path)\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "def read_image(path):\n",
        "  # path=path.decode()\n",
        "  x=cv2.imread(path,cv2.IMREAD_COLOR)\n",
        "  x=cv2.resize(x,(H,W))\n",
        "  x=x/255\n",
        "  x=x.astype(np.float32)\n",
        "  return x\n",
        "\n",
        "def read_mask(path):\n",
        "  # path=path.decode()\n",
        "  x=cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
        "  x=cv2.resize(x,(W,H))\n",
        "  x=x/255\n",
        "  x=x>0.5\n",
        "  x=x.astype(np.float32)\n",
        "  x=np.expand_dims(x,axis=-1)\n",
        "  return x\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "def tf_parse(x,y): \n",
        "  def _parse(x,y):\n",
        "    x=read_image(x)\n",
        "    y=read_image(y)\n",
        "    return x, y\n",
        "  x,y=tf.numpy_function(_parse,[x,y],[tf.float32,tf.float32])\n",
        "  x.set_shape([H,W,3])\n",
        "  y.set_shape([H,W,1])\n",
        "  return x,y \n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "def tf_dataset(X,  Y, batch=2):\n",
        "  dataset=tf.data.Dataset.from_tenor_slices((X,Y))\n",
        "  dataset=dataset.map(tf_parse)\n",
        "  dataset=dataset.batch(batch)\n",
        "  dataset=dataset.prefetch(10)\n",
        "  return dataset\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# train_dataset=tf_dataset(x_train,y_train,batch=16)\n",
        "# valid_dataset=tf_dataset(x_val,y_val,batch=16)\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "JHkbF9EF4uKc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EPMRD9LmJdVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800fc68a-a153-4fa7-bf8b-546dd5434a26",
        "id": "pLq5J0Z2JdqV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pLLXqxmJdqW"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "img=read_image(\"/content/Images/1.jpeg\")\n",
        "cv2_imshow(img*255)\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxs-0WnpuL-K"
      },
      "source": [
        "## Unzipping the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjo0ZTV6uLaU"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/Images.zip -d /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIlGyCveJdqW"
      },
      "source": [
        "## Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kELcJYhJdqX"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob \n",
        "import numpy as np\n",
        "X=[]\n",
        "y=[]\n",
        "images=sorted(glob.glob(\"/content/drive/MyDrive/Augmented/Images/Images/*\"))\n",
        "masks=sorted(glob.glob(\"/content/drive/MyDrive/Augmented/Masks/Masks/*\"))\n",
        "\n",
        "\n",
        "for image,mask in zip(images,masks):\n",
        "  img=cv2.imread(image)\n",
        "  img=np.array(img)\n",
        "  img=np.resize(img,(256,256,1))\n",
        "  X.append(img)\n",
        " \n",
        "  msk=cv2.imread(mask)\n",
        "  msk=np.array(msk)\n",
        "  msk=np.resize(msk,(256,256,1))\n",
        "  y.append(msk)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "# X_train=cv2.imread(\"/content/drive/MyDrive/Augmented/Images/0_0_4958.jpeg\")\n",
        "# X_train=np.resize(X_train,(1,256,256,3))\n",
        "# X_train=tf.expand_dims(X_train, axis=0)\n",
        "# y_train=cv2.imread(\"/content/drive/MyDrive/Augmented/Masks/0_0_4958.jpeg\")\n",
        "# y_train=np.resize(y_train,(1,256,256,1))\n",
        "# y_train=tf.expand_dims(y_train, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLozT-sMs9uM"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "# image_datagen = ImageDataGenerator(\n",
        "#                      rotation_range=40,\n",
        "#                      brightness_range=(0,5),zoom_range=0.2,\n",
        "#                      shear_range=0.2,\n",
        "#                      horizontal_flip=True,\n",
        "#                      vertical_flip=True,\n",
        "#                      fill_mode=\"nearest\",\n",
        "#                      ) # custom fuction for each image you can use resnet one too.\n",
        "\n",
        "# mask_datagen = ImageDataGenerator(\n",
        "#                      rotation_range=40,\n",
        "#                      brightness_range=(0,5),zoom_range=0.2,\n",
        "#                      shear_range=0.2,\n",
        "#                      horizontal_flip=True,\n",
        "#                      vertical_flip=True,\n",
        "#                      fill_mode=\"nearest\",\n",
        "#                      preprocessing_function=lambda x:np.where(x>0,1,0).astype()\n",
        "#                      ) # custom fuction for each image you can use resnet one too.\n",
        "\n",
        "# image_generator =image_datagen.flow_from_directory(\"/content/drive/MyDrive/Augmented/Images/\", class_mode=\"binary\",seed=42)\n",
        "# mask_generator = mask_datagen.flow_from_directory(\"/content/drive/MyDrive/Augmented/Masks/\", class_mode=\"binary\",seed=42)\n",
        "\n",
        "# train_generator = zip(image_generator, mask_generator)\n",
        "# train_generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X,y):\n",
        "  X=np.array(X)\n",
        "  y=np.array(y)\n",
        "  return X,y"
      ],
      "metadata": {
        "id": "jO0X4vyx1p41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X,y=preprocess(X,y)\n",
        "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.25,random_state=42)"
      ],
      "metadata": {
        "id": "3CdFj038rpnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ERROR CONVERT 5 DIMS TO 4 DIMS, EXTRA 1 DIM IS THERE"
      ],
      "metadata": {
        "id": "yeUmi-OR2DQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "seed=42\n",
        "image_data_gen_args=dict(rotation_range=90,\n",
        "                         zoom_range=0.3,\n",
        "                         horizontal_flip=True,\n",
        "                         vertical_flip=True,\n",
        "                         fill_mode=\"reflect\")\n",
        "\n",
        "mask_data_gen_args=dict(rotation_range=90,\n",
        "                         zoom_range=0.3,\n",
        "                         horizontal_flip=True,\n",
        "                         vertical_flip=True,\n",
        "                         fill_mode=\"reflect\",\n",
        "                         preprocessing_function=lambda x:np.where(x>0,1,0).astype(x.dtype))\n",
        "\n",
        "image_data_generator=ImageDataGenerator(**image_data_gen_args)\n",
        "image_data_generator.fit(X_train, augment=True, seed=seed)\n",
        "image_generator=image_data_generator.flow(X_train, seed=seed)\n",
        "valid_img_generator=image_data_generator.flow(X_test, seed=seed)\n",
        "\n",
        "mask_data_generator=ImageDataGenerator(**mask_data_gen_args)\n",
        "mask_data_generator.fit(y_train, seed=seed)\n",
        "mask_generator=mask_data_generator.flow(y_train, seed=seed)\n",
        "valid_mask_generator=mask_data_generator.flow(y_test, seed=seed)\n",
        "\n",
        "def my_image_mask_generator(image_generator, mask_generator):\n",
        "  train_generator=zip(image_generator, mask_generator)\n",
        "  for(img, mask) in train_generator:\n",
        "    yield (img, mask)\n",
        "\n",
        "my_generator = my_image_mask_generator(image_generator, mask_generator)\n",
        "validation_datagen = my_image_mask_generator(valid_img_generator, valid_mask_generator)"
      ],
      "metadata": {
        "id": "TZAaa2nsiLO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x=image_generator.next()\n",
        "y=mask_generator.next()\n",
        "for i in range(0,1):\n",
        "  image=x[i]\n",
        "  mask=y[i]\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(image[:,:,0],cmap=\"gray\")\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(mask[:,:,0])\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "HgD7XNbf3f5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVkmr4YGJdqY"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjdfGf5-JdqY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(inputs,num_filters):\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(inputs)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def encoder_block(inputs,num_filters):\n",
        "  x=conv_block(inputs,num_filters)\n",
        "  p=MaxPool2D((2,2))(x)\n",
        "  return x,p\n",
        "\n",
        "def decoder_block(inputs, skip_features,num_filters):\n",
        "  x=Conv2DTranspose(num_filters,(2,2),strides=2,padding=\"same\")(inputs)\n",
        "  x=Concatenate()([x,skip_features])\n",
        "  x=conv_block(x,num_filters)\n",
        "  return x\n",
        "\n",
        "def unet(input_shape):\n",
        "  inputs=Input(input_shape)\n",
        "\n",
        "  s1,p1=encoder_block(inputs,64)\n",
        "  s2,p2=encoder_block(p1,128)\n",
        "  s3,p3=encoder_block(p2,256)\n",
        "  s4,p4=encoder_block(p3,512)\n",
        "\n",
        "  b1 = conv_block(p4,1024)\n",
        "\n",
        "  d1 = decoder_block(b1,s4,512)\n",
        "  d2 = decoder_block(d1,s3,256)\n",
        "  d3 = decoder_block(d2,s2,128)\n",
        "  d4 = decoder_block(d3,s1,64)\n",
        "\n",
        "  outputs=Conv2D(1,(1,1),padding=\"same\",activation=\"sigmoid\")(d4)\n",
        "  model=Model(inputs,outputs,name=\"U-Net\")\n",
        "  return model  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8moIQEoJdqY"
      },
      "outputs": [],
      "source": [
        "# Model Summary\n",
        "model=unet((256,256,3))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEykoegwJdqZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "input_shape=(256,256,3)\n",
        "model=unet(input_shape)\n",
        "model.compile(tf.keras.optimizers.RMSprop(1e-4),loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "batch_size=64\n",
        "model.fit_generator(my_generator, validation_data=validation_datagen, steps_per_epoch=32, validation_steps=50, epochs=100,callbacks=tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\"))\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# model.compile()\n",
        "# model.compile(keras.optimizers.Adam(),loss=jaccard_loss,metrics=iou_metrics)\n",
        "# model.compile(loss=dice_loss, optimizer=Adam(1e-4),metrics=[dice_coef])\n",
        "# model.compile(loss=dice_loss,optimizer=Adam(),metrics=metrics)\n",
        "# metrics=[dice_coef,iou,Recall(),Precision()]\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# Callbacks\n",
        "# callbacks=[tf.keras.callbacks.EarlyStopping(patience=15,monitor=\"val_loss\"),\n",
        "#             tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\"),\n",
        "#             tf.keras.callbacks.ModelCheckpoint(\"model_for_balls.h5\",save_best_only=True)\n",
        "#             tf.keras.callbacks.ReduceLROnPlateau()]\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# model.fit()\n",
        "# results=model.fit(X_train,y_train,validation_split=0.1,batch_size=16,epochs=100,callbacks=callbacks)\n",
        "# model.fit(train_dataset,epochs=20,validation_data=valid_dataset,callbacks=callbacks)\n",
        "# val_generator=zip(X_val,y_val)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "#ReduceLROnPlateau[49:58]: https://www.youtube.com/watch?v=lstBIXVUoSM\n",
        "# model.save(\"/content/drive/MyDrive/Birds_Dataset_Main/Models/VGG19_Adam.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install segmentation_models\n",
        "import segmentation_models as sm\n",
        "jaccard_loss=sm.losses.bce_jaccard_loss\n",
        "iou_metrics=[sm.metrics.iou_score]"
      ],
      "metadata": {
        "id": "XuuthuMfohT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "test=cv2.imread(\"/content/Images/Images /3.jpeg\")\n",
        "original_shape=test.shape\n",
        "test=np.resize(test,(256,256,3))\n",
        "\n",
        "ground_truth=cv2.imread(\"/content/drive/MyDrive/Bitmaps/Masks /3.jpeg\")\n",
        "ground_truth=np.resize(ground_truth, (168, 300, 3))\n",
        "\n",
        "test_img_input=np.expand_dims(test,0)\n",
        "\n",
        "prediction=model.predict(test_img_input)\n",
        "prediction=(prediction>0.4)*255\n",
        "\n",
        "# prediction=np.array(prediction)\n",
        "prediction=np.resize(prediction, (168, 300, 1))\n",
        "prediction=prediction[0:,:,0]\n",
        "# prediction=np.resize(prediction,(177, 284,2))\n",
        "test=np.resize(test,(168, 300, 3))\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.subplot(231)\n",
        "plt.title(\"Testing image\")\n",
        "plt.imshow(test[:,:,0],cmap=\"gray\")\n",
        "plt.subplot(232)\n",
        "plt.title(\"Testing Label\")\n",
        "plt.imshow(ground_truth[:,:,0],cmap=\"gray\")\n",
        "plt.subplot(233)\n",
        "plt.title(\"Prediction\")\n",
        "plt.imshow(prediction,cmap=\"gray\")"
      ],
      "metadata": {
        "id": "cdZwaCDr4GlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI3hNGLW7NuT",
        "outputId": "c9f5f3af-f138-4238-b6da-2e50b7652d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(168, 300, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdSushUZTY_F"
      },
      "outputs": [],
      "source": [
        "def generator(images, masks):      \n",
        "  train_generator = zip(images, masks)\n",
        "  for (images, masks) in train_generator:\n",
        "    yield (images, masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSa0VC1OR7ft"
      },
      "outputs": [],
      "source": [
        "# def my_image_mask_generator(image_data_generator, mask_data_generator):\n",
        "#     train_generator = zip(image_data_generator, mask_data_generator)\n",
        "#     for (img, mask) in train_generator:\n",
        "#         yield (img, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI91SML2fjA2"
      },
      "outputs": [],
      "source": [
        "# train_generator=tf.data.Dataset.from_tensor_slices(X_train,y_train).batch(12, drop_remainder=True).repeat(count=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oslnDMb3Jdqa"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HiIcMnkJdqa"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/logs"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "226A3CbpnXv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSTDnpeINfHE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWfQS_WdNfHF"
      },
      "source": [
        "## Unzipping the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHjJzWJKNfHF"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/Images.zip -d /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfJwPfoJNfHG"
      },
      "source": [
        "## Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6VPmFUpNfHG"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob \n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "X=[]\n",
        "y=[]\n",
        "images=sorted(glob.glob(\"/content/drive/MyDrive/Augmented/Images/Images/*\"))\n",
        "masks=sorted(glob.glob(\"/content/drive/MyDrive/Augmented/Masks/Masks/*\"))\n",
        "\n",
        "for image,mask in zip(images,masks):\n",
        "  image=cv2.imread(image)\n",
        "  image=cv2.resize(image, (256,256))\n",
        "  image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  image=np.expand_dims(image,axis=-1)\n",
        "  image=image/255\n",
        "  X.append(image)\n",
        "\n",
        "  mask=cv2.imread(mask)\n",
        "  mask=cv2.resize(mask, (256,256))\n",
        "  mask=cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "  mask=np.expand_dims(mask,axis=-1)\n",
        "  mask=mask/255\n",
        "  mask=np.where(mask>0.5,1,0).astype(mask.dtype)\n",
        "  y.append(mask)\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X,y):\n",
        "  X=np.array(X)\n",
        "  y=np.array(y)\n",
        "  return X,y"
      ],
      "metadata": {
        "id": "juRtN9LBnYWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X,y=preprocess(X,y)\n",
        "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.25,random_state=42)"
      ],
      "metadata": {
        "id": "VmTrl_banYWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmxsFQtJ_n0v",
        "outputId": "9991ee89-81fb-48a8-b92b-97980f7777d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45, 256, 256, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ERROR CONVERT 5 DIMS TO 4 DIMS, EXTRA 1 DIM IS THERE"
      ],
      "metadata": {
        "id": "fgSyZrqvnYWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "seed=42\n",
        "image_data_gen_args=dict(rotation_range=90,\n",
        "                         zoom_range=0.3,\n",
        "                         horizontal_flip=True,\n",
        "                         vertical_flip=True,\n",
        "                         fill_mode=\"reflect\")\n",
        "\n",
        "mask_data_gen_args=dict(rotation_range=90,\n",
        "                         zoom_range=0.3,\n",
        "                         horizontal_flip=True,\n",
        "                         vertical_flip=True,\n",
        "                         fill_mode=\"reflect\",\n",
        "                         preprocessing_function=lambda x:np.where(x>0,1,0).astype(x.dtype))\n",
        "\n",
        "image_data_generator=ImageDataGenerator(**image_data_gen_args)\n",
        "image_data_generator.fit(X_train, augment=True, seed=seed)\n",
        "image_generator=image_data_generator.flow(X_train, seed=seed)\n",
        "valid_img_generator=image_data_generator.flow(X_test, seed=seed)\n",
        "\n",
        "mask_data_generator=ImageDataGenerator(**mask_data_gen_args)\n",
        "mask_data_generator.fit(y_train, seed=seed)\n",
        "mask_generator=mask_data_generator.flow(y_train, seed=seed)\n",
        "valid_mask_generator=mask_data_generator.flow(y_test, seed=seed)\n",
        "\n",
        "def my_image_mask_generator(image_generator, mask_generator):\n",
        "  train_generator=zip(image_generator, mask_generator)\n",
        "  for(img, mask) in train_generator:\n",
        "    yield (img, mask)\n",
        "\n",
        "my_generator = my_image_mask_generator(image_generator, mask_generator)\n",
        "validation_datagen = my_image_mask_generator(valid_img_generator, valid_mask_generator)"
      ],
      "metadata": {
        "id": "ERvngHtfnYWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x=image_generator.next()\n",
        "y=mask_generator.next()\n",
        "for i in range(0,1):\n",
        "  image=x[i]\n",
        "  mask=y[i]\n",
        "  mask=mask\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(image[:,:,0],cmap=\"gray\")\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(mask[:,:,0])\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "yyF68la-nYWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBiDdkHvnYWY"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgIKNosHnYWY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(inputs,num_filters):\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(inputs)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "\n",
        "  x=Conv2D(num_filters,3,padding=\"same\")(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Activation(\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def encoder_block(inputs,num_filters):\n",
        "  x=conv_block(inputs,num_filters)\n",
        "  p=MaxPool2D((2,2))(x)\n",
        "  return x,p\n",
        "\n",
        "def decoder_block(inputs, skip_features,num_filters):\n",
        "  x=Conv2DTranspose(num_filters,(2,2),strides=2,padding=\"same\")(inputs)\n",
        "  x=Concatenate()([x,skip_features])\n",
        "  x=conv_block(x,num_filters)\n",
        "  return x\n",
        "\n",
        "def unet(input_shape):\n",
        "  inputs=Input(input_shape)\n",
        "\n",
        "  s1,p1=encoder_block(inputs,64)\n",
        "  s2,p2=encoder_block(p1,128)\n",
        "  s3,p3=encoder_block(p2,256)\n",
        "  s4,p4=encoder_block(p3,512)\n",
        "\n",
        "  b1 = conv_block(p4,1024)\n",
        "\n",
        "  d1 = decoder_block(b1,s4,512)\n",
        "  d2 = decoder_block(d1,s3,256)\n",
        "  d3 = decoder_block(d2,s2,128)\n",
        "  d4 = decoder_block(d3,s1,64)\n",
        "\n",
        "  outputs=Conv2D(1,(1,1),padding=\"same\",activation=\"sigmoid\")(d4)\n",
        "  model=Model(inputs,outputs,name=\"U-Net\")\n",
        "  return model  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtWLEApJnYWY"
      },
      "outputs": [],
      "source": [
        "# Model Summary\n",
        "model=unet((None,None,3))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks=[\n",
        "    # tf.keras.callbacks.EarlyStopping(patience=8,monitor=\"accuracy\"),\n",
        "            tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\"),\n",
        "            tf.keras.callbacks.ModelCheckpoint(\"/content/model_weights/model_for_balls.h5\",save_best_only=True),   \n",
        "            tf.keras.callbacks.ReduceLROnPlateau(monitor=\"accuracy\", factor=0.2, patience=5, min_lr=0.001)]\n"
      ],
      "metadata": {
        "id": "TqCGA2F6PcuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaB-H9umnYWY"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "# model = keras.models.load_model('/content/model_weights/model_for_balls.h5')\n",
        "input_shape=(256,256,1)\n",
        "model=unet(input_shape)\n",
        "model.compile(tf.keras.optimizers.RMSprop(1e-3),loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# model.compile()\n",
        "# model.compile(keras.optimizers.Adam(),loss=jaccard_loss,metrics=iou_metrics)\n",
        "# model.compile(loss=dice_loss, optimizer=Adam(1e-4),metrics=[dice_coef])\n",
        "# model.compile(loss=dice_loss,optimizer=Adam(),metrics=metrics)\n",
        "# metrics=[dice_coef,iou,Recall(),Precision()]\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# Callbacks\n",
        "# ]\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# model.fit()\n",
        "# results=model.fit(X_train,y_train,validation_split=0.1,batch_size=16,epoc hs=100,callbacks=callbacks)\n",
        "# model.fit(train_dataset,epochs=20,validation_data=valid_dataset,callbacks=callbacks)\n",
        "# val_generator=zip(X_val,y_val)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "#ReduceLROnPlateau[49:58]: https://www.youtube.com/watch?v=lstBIXVUoSM\n",
        "# model.save(\"/content/drive/MyDrive/Birds_Dataset_Main/Models/VGG19_Adam.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting The model\n",
        "model.fit_generator(my_generator, validation_data=validation_datagen, steps_per_epoch=16, validation_steps=50, epochs=100, callbacks=callbacks)\n",
        "\n",
        "# Saving the model\n",
        "!mkdir /content/drive/MyDrive/Unet_Weights\n",
        "model.save(\"/content/drive/MyDrive/Unet_Weights/31_Mil.h5\")"
      ],
      "metadata": {
        "id": "-A_SfIcieyCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/Unet_Weights\n",
        "model.save(\"/content/drive/MyDrive/Unet_Weights/31_Mil.h5\")"
      ],
      "metadata": {
        "id": "z61hu7QBvuQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Model"
      ],
      "metadata": {
        "id": "1tjHu7w3vY8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model= load_model(\"/content/drive/MyDrive/Unet_Weights/31_Mil.h5\")"
      ],
      "metadata": {
        "id": "iylvgTNtuVCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Summary"
      ],
      "metadata": {
        "id": "OG-hB15PwjkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "GqwlOD8Jwjas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Inferencing "
      ],
      "metadata": {
        "id": "doDBbMpmwsKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "# Test Image\n",
        "test_image=cv2.imread(\"/content/download.jpg\")\n",
        "test_image=cv2.resize(test_image, (256,256))\n",
        "test_image=cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
        "test_image=test_image/255\n",
        "test_image=np.expand_dims(test_image,axis=-1)\n",
        "test_image=np.expand_dims(test_image,0)\n",
        "# Masking Prediction\n",
        "prediction=model.predict(test_image)\n",
        "prediction=np.where(prediction>0.3,1,0).astype(prediction.dtype)\n",
        "\n",
        "# Original Mask\n",
        "original_mask=cv2.imread(\"/content/drive/MyDrive/Augmented/Masks/Masks/0_0_4958.jpeg\")\n",
        "original_mask=cv2.resize(original_mask, (256,256))\n",
        "original_mask=cv2.cvtColor(original_mask, cv2.COLOR_BGR2GRAY)\n",
        "original_mask=np.expand_dims(original_mask,axis=-1)\n",
        "original_mask=np.expand_dims(original_mask,0)\n",
        "original_mask=original_mask/255\n",
        "original_mask=np.where(original_mask>0.5,1,0).astype(original_mask.dtype)\n",
        "\n",
        "# print(test_image.shape)\n",
        "# print(prediction.shape)\n",
        "# print(original_mask.shape)"
      ],
      "metadata": {
        "id": "9NCxTU1nwwnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "test_image=np.resize(test_image,(256,256,1))\n",
        "original_mask=np.resize(original_mask,(256,256,1))\n",
        "prediction=np.resize(prediction,(256,256,1))\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------\n",
        "plt.subplot(231)\n",
        "plt.title(\"Test Image\")\n",
        "plt.imshow(test_image[:,:,0],cmap=\"gray\")\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------\n",
        "plt.subplot(232)\n",
        "plt.title(\"Original_Mask\")\n",
        "plt.imshow(original_mask[:,:,0],cmap=\"gray\")\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------\n",
        "plt.subplot(233)\n",
        "plt.title(\"Prediction\")\n",
        "plt.imshow(prediction[:,:,0],cmap=\"gray\")\n",
        "# --------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "56d0d40f-ce0b-4978-e008-9f029b2b1e2f",
        "id": "WFYpvwqJnYWZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3c277ef490>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test=cv2.imread(\"/content/drive/MyDrive/Augmented/Test_Images/4_0_4958.jpeg\")\n",
        "test=np.resize(test,(256,256,3))\n",
        "\n",
        "# ground_truth=cv2.imread(\"/content/drive/MyDrive/Augmented/Test_Masks/4_0_4958.jpeg\")\n",
        "# ground_truth=np.resize(ground_truth, (168, 300, 1))\n",
        "\n",
        "test=np.expand_dims(test,0)\n",
        "prediction=model.predict(test)\n",
        "prediction=prediction>0.5\n",
        "prediction=np.resize(prediction,(168,168,3))\n",
        "# prediction=(prediction)\n",
        "plt.imshow(prediction)\n",
        "# test_img_input.shape"
      ],
      "metadata": {
        "id": "brCHP9TMzPAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9tKiGybnYWa"
      },
      "outputs": [],
      "source": [
        "def generator(images, masks):      \n",
        "  train_generator = zip(images, masks)\n",
        "  for (images, masks) in train_generator:\n",
        "    yield (images, masks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install segmentation_models\n",
        "import segmentation_models as sm\n",
        "jaccard_loss=sm.losses.bce_jaccard_loss\n",
        "iou_metrics=[sm.metrics.iou_score]"
      ],
      "metadata": {
        "id": "Fu8H5efZnYWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te1EKrDjnYWa"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MayOeoYznYWa"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/logs"
      ]
    }
  ]
}